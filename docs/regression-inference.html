<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Data Analysis Notes</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>">
  <meta name="generator" content="bookdown 0.3.6 and GitBook 2.6.7">

  <meta property="og:title" content="Data Analysis Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  <meta name="github-repo" content="jrnold/intro-methods-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Data Analysis Notes" />
  
  <meta name="twitter:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  

<meta name="author" content="Jeffrey B. Arnold">


<meta name="date" content="2017-04-19">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="interpreting-coefficients.html">
<link rel="next" href="omitted-variable-bias.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<link href="libs/plotlyjs-1.16.3/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.16.3/plotly-latest.min.js"></script>
<script src="libs/plotly-binding-4.5.6/plotly.js"></script>



<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intro Method Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="part"><span><b>I Exploratory Data Analysis</b></span></li>
<li class="part"><span><b>II Probability</b></span></li>
<li class="part"><span><b>III Inference</b></span></li>
<li class="part"><span><b>IV Linear Regresssion</b></span></li>
<li class="chapter" data-level="2" data-path="bivariate-ols.html"><a href="bivariate-ols.html"><i class="fa fa-check"></i><b>2</b> Bivariate OLS</a><ul>
<li class="chapter" data-level="2.0.1" data-path="bivariate-ols.html"><a href="bivariate-ols.html#ols-is-the-weighted-sum-of-outcomes"><i class="fa fa-check"></i><b>2.0.1</b> OLS is the weighted sum of outcomes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html"><i class="fa fa-check"></i><b>3</b> Goodness of Fit</a><ul>
<li class="chapter" data-level="3.1" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html#root-mean-squared-error-and-standard-error"><i class="fa fa-check"></i><b>3.1</b> Root Mean Squared Error and Standard Error</a></li>
<li class="chapter" data-level="3.2" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html#r-squared"><i class="fa fa-check"></i><b>3.2</b> R squared</a></li>
<li class="chapter" data-level="3.3" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html#maximum-likelihood"><i class="fa fa-check"></i><b>3.3</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="3.4" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html#regression-line"><i class="fa fa-check"></i><b>3.4</b> Regression Line</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>4</b> Multiple Regression</a></li>
<li class="chapter" data-level="5" data-path="what-is-regression.html"><a href="what-is-regression.html"><i class="fa fa-check"></i><b>5</b> What is Regression?</a><ul>
<li class="chapter" data-level="5.1" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-regression-and-what-is-it-used-for"><i class="fa fa-check"></i><b>5.1</b> What is Regression and What is it Used For ?</a></li>
<li class="chapter" data-level="5.2" data-path="what-is-regression.html"><a href="what-is-regression.html#joint-vs.conditional-models"><i class="fa fa-check"></i><b>5.2</b> Joint vs. Conditional models</a></li>
<li class="chapter" data-level="5.3" data-path="what-is-regression.html"><a href="what-is-regression.html#conditional-expectation-function"><i class="fa fa-check"></i><b>5.3</b> Conditional expectation function</a><ul>
<li class="chapter" data-level="5.3.1" data-path="what-is-regression.html"><a href="what-is-regression.html#discrete-covariates"><i class="fa fa-check"></i><b>5.3.1</b> Discrete Covariates</a></li>
<li class="chapter" data-level="5.3.2" data-path="what-is-regression.html"><a href="what-is-regression.html#continuous-covariates"><i class="fa fa-check"></i><b>5.3.2</b> Continuous Covariates</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="interpreting-coefficients.html"><a href="interpreting-coefficients.html"><i class="fa fa-check"></i><b>6</b> Interpreting Coefficients</a><ul>
<li class="chapter" data-level="6.1" data-path="interpreting-coefficients.html"><a href="interpreting-coefficients.html#interactions-and-polynomials"><i class="fa fa-check"></i><b>6.1</b> Interactions and Polynomials</a></li>
<li class="chapter" data-level="6.2" data-path="interpreting-coefficients.html"><a href="interpreting-coefficients.html#average-marginal-effects"><i class="fa fa-check"></i><b>6.2</b> Average Marginal Effects</a></li>
<li class="chapter" data-level="6.3" data-path="interpreting-coefficients.html"><a href="interpreting-coefficients.html#standardized-coefficients"><i class="fa fa-check"></i><b>6.3</b> Standardized Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="regression-inference.html"><a href="regression-inference.html"><i class="fa fa-check"></i><b>7</b> Regression Inference</a><ul>
<li class="chapter" data-level="7.1" data-path="regression-inference.html"><a href="regression-inference.html#prerequisites"><i class="fa fa-check"></i><b>7.1</b> Prerequisites</a></li>
<li class="chapter" data-level="7.2" data-path="regression-inference.html"><a href="regression-inference.html#sampling-distribution-and-standard-errors-of-coefficients"><i class="fa fa-check"></i><b>7.2</b> Sampling Distribution and Standard Errors of Coefficients</a></li>
<li class="chapter" data-level="7.3" data-path="regression-inference.html"><a href="regression-inference.html#single-coefficient"><i class="fa fa-check"></i><b>7.3</b> Single Coefficient</a><ul>
<li class="chapter" data-level="7.3.1" data-path="regression-inference.html"><a href="regression-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>7.3.1</b> Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="regression-inference.html"><a href="regression-inference.html#multiple-coefficients"><i class="fa fa-check"></i><b>7.4</b> Multiple Coefficients</a><ul>
<li class="chapter" data-level="7.4.1" data-path="regression-inference.html"><a href="regression-inference.html#f-test"><i class="fa fa-check"></i><b>7.4.1</b> F-test</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="regression-inference.html"><a href="regression-inference.html#confidence-ellipses"><i class="fa fa-check"></i><b>7.5</b> Confidence Ellipses</a></li>
<li class="chapter" data-level="7.6" data-path="regression-inference.html"><a href="regression-inference.html#linear-hypothesis-tests"><i class="fa fa-check"></i><b>7.6</b> Linear Hypothesis Tests</a></li>
<li class="chapter" data-level="7.7" data-path="regression-inference.html"><a href="regression-inference.html#non-linear-hypothesis-coefficients"><i class="fa fa-check"></i><b>7.7</b> Non-Linear Hypothesis Coefficients</a></li>
<li class="chapter" data-level="7.8" data-path="regression-inference.html"><a href="regression-inference.html#confidence-intervals-for-linear-and-non-linear-functions-of-coefficients"><i class="fa fa-check"></i><b>7.8</b> Confidence Intervals for Linear and Non-linear functions of Coefficients</a></li>
<li class="chapter" data-level="7.9" data-path="regression-inference.html"><a href="regression-inference.html#multiple-testing"><i class="fa fa-check"></i><b>7.9</b> Multiple Testing</a></li>
<li class="chapter" data-level="7.10" data-path="regression-inference.html"><a href="regression-inference.html#data-snooping"><i class="fa fa-check"></i><b>7.10</b> Data snooping</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html"><i class="fa fa-check"></i><b>8</b> Omitted Variable Bias</a><ul>
<li class="chapter" data-level="8.1" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#prerequisites-1"><i class="fa fa-check"></i><b>8.1</b> Prerequisites</a></li>
<li class="chapter" data-level="8.2" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#simpsons-paradox"><i class="fa fa-check"></i><b>8.2</b> Simpson’s Paradox</a></li>
<li class="chapter" data-level="8.3" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#omitted-variable-bias-1"><i class="fa fa-check"></i><b>8.3</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="8.4" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#measurement-error"><i class="fa fa-check"></i><b>8.4</b> Measurement Error</a><ul>
<li class="chapter" data-level="8.4.1" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#whats-the-problem"><i class="fa fa-check"></i><b>8.4.1</b> What’s the problem?</a></li>
<li class="chapter" data-level="8.4.2" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#what-to-do-about-it"><i class="fa fa-check"></i><b>8.4.2</b> What to do about it?</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#more-information"><i class="fa fa-check"></i><b>8.5</b> More Information</a><ul>
<li class="chapter" data-level="8.5.1" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#simpsons-paradox-1"><i class="fa fa-check"></i><b>8.5.1</b> Simpson’s Paradox</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="outliers.html"><a href="outliers.html"><i class="fa fa-check"></i><b>9</b> Outliers</a></li>
<li class="chapter" data-level="10" data-path="problems-with-errors.html"><a href="problems-with-errors.html"><i class="fa fa-check"></i><b>10</b> Problems with Errors</a><ul>
<li class="chapter" data-level="10.1" data-path="problems-with-errors.html"><a href="problems-with-errors.html#prerequisites-2"><i class="fa fa-check"></i><b>10.1</b> Prerequisites</a></li>
<li class="chapter" data-level="10.2" data-path="problems-with-errors.html"><a href="problems-with-errors.html#heteroskedasticity"><i class="fa fa-check"></i><b>10.2</b> Heteroskedasticity</a><ul>
<li class="chapter" data-level="10.2.1" data-path="problems-with-errors.html"><a href="problems-with-errors.html#example-duncans-occupation-data"><i class="fa fa-check"></i><b>10.2.1</b> Example: Duncan’s Occupation Data</a></li>
<li class="chapter" data-level="10.2.2" data-path="problems-with-errors.html"><a href="problems-with-errors.html#notes"><i class="fa fa-check"></i><b>10.2.2</b> Notes</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="problems-with-errors.html"><a href="problems-with-errors.html#correlated-errors"><i class="fa fa-check"></i><b>10.3</b> Correlated Errors</a></li>
<li class="chapter" data-level="10.4" data-path="problems-with-errors.html"><a href="problems-with-errors.html#non-normal-errors"><i class="fa fa-check"></i><b>10.4</b> Non-normal Errors</a></li>
<li class="chapter" data-level="10.5" data-path="problems-with-errors.html"><a href="problems-with-errors.html#bootstrapping"><i class="fa fa-check"></i><b>10.5</b> Bootstrapping</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="weighted-regression.html"><a href="weighted-regression.html"><i class="fa fa-check"></i><b>11</b> Weighted Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="weighted-regression.html"><a href="weighted-regression.html#weighted-least-squares-wls"><i class="fa fa-check"></i><b>11.1</b> Weighted Least Squares (WLS)</a></li>
<li class="chapter" data-level="11.2" data-path="weighted-regression.html"><a href="weighted-regression.html#when-should-you-use-wls"><i class="fa fa-check"></i><b>11.2</b> When should you use WLS?</a></li>
<li class="chapter" data-level="11.3" data-path="weighted-regression.html"><a href="weighted-regression.html#correcting-for-known-heteroskedasticity"><i class="fa fa-check"></i><b>11.3</b> Correcting for Known Heteroskedasticity</a></li>
<li class="chapter" data-level="11.4" data-path="weighted-regression.html"><a href="weighted-regression.html#sampling-weights"><i class="fa fa-check"></i><b>11.4</b> Sampling Weights</a></li>
<li class="chapter" data-level="11.5" data-path="weighted-regression.html"><a href="weighted-regression.html#references"><i class="fa fa-check"></i><b>11.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="discrete-outcome-variables.html"><a href="discrete-outcome-variables.html"><i class="fa fa-check"></i><b>12</b> Discrete Outcome Variables</a><ul>
<li class="chapter" data-level="12.1" data-path="discrete-outcome-variables.html"><a href="discrete-outcome-variables.html#linear-probability-model"><i class="fa fa-check"></i><b>12.1</b> Linear Probability Model</a></li>
<li class="chapter" data-level="12.2" data-path="discrete-outcome-variables.html"><a href="discrete-outcome-variables.html#logit-model"><i class="fa fa-check"></i><b>12.2</b> Logit Model</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="robust-regression.html"><a href="robust-regression.html"><i class="fa fa-check"></i><b>13</b> Robust Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="robust-regression.html"><a href="robust-regression.html#prerequites"><i class="fa fa-check"></i><b>13.1</b> Prerequites</a></li>
<li class="chapter" data-level="13.2" data-path="robust-regression.html"><a href="robust-regression.html#examples"><i class="fa fa-check"></i><b>13.2</b> Examples</a></li>
<li class="chapter" data-level="13.3" data-path="robust-regression.html"><a href="robust-regression.html#notes-1"><i class="fa fa-check"></i><b>13.3</b> Notes</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html"><i class="fa fa-check"></i><b>14</b> Prediction and Model Comparison</a><ul>
<li class="chapter" data-level="14.1" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#prerequisites-3"><i class="fa fa-check"></i><b>14.1</b> Prerequisites</a></li>
<li class="chapter" data-level="14.2" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#measures-of-prediction"><i class="fa fa-check"></i><b>14.2</b> Measures of Prediction</a></li>
<li class="chapter" data-level="14.3" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#model-comparison"><i class="fa fa-check"></i><b>14.3</b> Model Comparison</a></li>
<li class="chapter" data-level="14.4" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#example-predicting-the-price-of-wine"><i class="fa fa-check"></i><b>14.4</b> Example: Predicting the Price of Wine</a></li>
<li class="chapter" data-level="14.5" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#cross-validation"><i class="fa fa-check"></i><b>14.5</b> Cross-Validation</a></li>
<li class="chapter" data-level="14.6" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#out-of-sample-error"><i class="fa fa-check"></i><b>14.6</b> Out of Sample Error</a><ul>
<li class="chapter" data-level="14.6.1" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#held-out-data"><i class="fa fa-check"></i><b>14.6.1</b> Held-out data</a></li>
<li class="chapter" data-level="14.6.2" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>14.6.2</b> Leave-One-Out Cross-Validation</a></li>
<li class="chapter" data-level="14.6.3" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>14.6.3</b> k-fold Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#analytic-covariance-methods"><i class="fa fa-check"></i><b>14.7</b> Analytic Covariance Methods</a></li>
<li class="chapter" data-level="14.8" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#further-resources"><i class="fa fa-check"></i><b>14.8</b> Further Resources</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="miscellaneous-regression-stuff.html"><a href="miscellaneous-regression-stuff.html"><i class="fa fa-check"></i><b>15</b> Miscellaneous Regression Stuff</a><ul>
<li class="chapter" data-level="15.1" data-path="miscellaneous-regression-stuff.html"><a href="miscellaneous-regression-stuff.html#anscombe-quartet"><i class="fa fa-check"></i><b>15.1</b> Anscombe quartet</a></li>
<li class="chapter" data-level="15.2" data-path="miscellaneous-regression-stuff.html"><a href="miscellaneous-regression-stuff.html#correlation-plots"><i class="fa fa-check"></i><b>15.2</b> Correlation Plots</a></li>
</ul></li>
<li class="part"><span><b>V Programming</b></span></li>
<li class="chapter" data-level="16" data-path="rs-forumula-syntax.html"><a href="rs-forumula-syntax.html"><i class="fa fa-check"></i><b>16</b> R’s Forumula Syntax</a><ul>
<li class="chapter" data-level="16.1" data-path="rs-forumula-syntax.html"><a href="rs-forumula-syntax.html#setup"><i class="fa fa-check"></i><b>16.1</b> Setup</a></li>
<li class="chapter" data-level="16.2" data-path="rs-forumula-syntax.html"><a href="rs-forumula-syntax.html#introduction-to-formula-objects"><i class="fa fa-check"></i><b>16.2</b> Introduction to Formula Objects</a></li>
<li class="chapter" data-level="16.3" data-path="rs-forumula-syntax.html"><a href="rs-forumula-syntax.html#programming-with-formulas"><i class="fa fa-check"></i><b>16.3</b> Programming with Formulas</a></li>
</ul></li>
<li class="part"><span><b>VI Examples</b></span></li>
<li class="chapter" data-level="17" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html"><i class="fa fa-check"></i><b>17</b> Duncan Occupational Prestige</a><ul>
<li class="chapter" data-level="17.1" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#setup-1"><i class="fa fa-check"></i><b>17.1</b> Setup</a></li>
<li class="chapter" data-level="17.2" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#coefficients-standard-errors"><i class="fa fa-check"></i><b>17.2</b> Coefficients, Standard errors</a></li>
<li class="chapter" data-level="17.3" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#residuals-fitted-values"><i class="fa fa-check"></i><b>17.3</b> Residuals, Fitted Values,</a></li>
<li class="chapter" data-level="17.4" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#broom"><i class="fa fa-check"></i><b>17.4</b> Broom</a></li>
<li class="chapter" data-level="17.5" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#plotting-fitted-regression-results"><i class="fa fa-check"></i><b>17.5</b> Plotting Fitted Regression Results</a></li>
</ul></li>
<li class="part"><span><b>VII Presentation</b></span></li>
<li class="chapter" data-level="18" data-path="formatting-tables.html"><a href="formatting-tables.html"><i class="fa fa-check"></i><b>18</b> Formatting Tables</a><ul>
<li class="chapter" data-level="18.1" data-path="formatting-tables.html"><a href="formatting-tables.html#overview-of-packages"><i class="fa fa-check"></i><b>18.1</b> Overview of Packages</a></li>
<li class="chapter" data-level="18.2" data-path="formatting-tables.html"><a href="formatting-tables.html#summary-statistic-table-example"><i class="fa fa-check"></i><b>18.2</b> Summary Statistic Table Example</a></li>
<li class="chapter" data-level="18.3" data-path="formatting-tables.html"><a href="formatting-tables.html#regression-table-example"><i class="fa fa-check"></i><b>18.3</b> Regression Table Example</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="reproducible-research.html"><a href="reproducible-research.html"><i class="fa fa-check"></i><b>19</b> Reproducible Research</a></li>
<li class="chapter" data-level="20" data-path="writing-resources.html"><a href="writing-resources.html"><i class="fa fa-check"></i><b>20</b> Writing Resources</a><ul>
<li class="chapter" data-level="20.1" data-path="writing-resources.html"><a href="writing-resources.html#writing-and-organizing-papers"><i class="fa fa-check"></i><b>20.1</b> Writing and Organizing Papers</a></li>
<li class="chapter" data-level="20.2" data-path="writing-resources.html"><a href="writing-resources.html#finding-research-ideas"><i class="fa fa-check"></i><b>20.2</b> Finding Research Ideas</a></li>
<li class="chapter" data-level="20.3" data-path="writing-resources.html"><a href="writing-resources.html#replications"><i class="fa fa-check"></i><b>20.3</b> Replications</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="multivariate-normal-distribution.html"><a href="multivariate-normal-distribution.html"><i class="fa fa-check"></i><b>A</b> Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
\[
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\RMSE}{RMSE}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\T}{'}

\newcommand{\distr}[1]{\mathcal{#1}}
\newcommand{\dnorm}{\distr{N}}
\newcommand{\dmvnorm}[1]{\distr{N}_{#1}}
\newcommand{\dt}[1]{\distr{T}_{#1}}

\newcommand{\cia}{\perp\!\!\!\perp}
\DeclareMathOperator*{\plim}{plim}
\]
<div id="regression-inference" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Regression Inference</h1>
<div id="prerequisites" class="section level2">
<h2><span class="header-section-number">7.1</span> Prerequisites</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;tidyverse&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;broom&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;stringr&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;magrittr&quot;</span>)</code></pre></div>
</div>
<div id="sampling-distribution-and-standard-errors-of-coefficients" class="section level2">
<h2><span class="header-section-number">7.2</span> Sampling Distribution and Standard Errors of Coefficients</h2>
<p>The standard error of a single regression coefficient is <span class="citation">(Fox <a href="#ref-Fox2008a">2008</a>, 107)</span> <span class="math display">\[
\widehat{\se}(\hat{\beta}_j) = \frac{1}{\sqrt{1 - R_j^2}} \times \frac{\hat{\sigma}^2}{\sum_i (x_{ij} - \bar{x}_j)^2} ,
\]</span> where <span class="math inline">\(R_j^2\)</span> is the <span class="math inline">\(R^2\)</span> of the linear regression of <span class="math inline">\(x_j\)</span> on all the other predictors except <span class="math inline">\(x_j\)</span>.</p>
<p>The first term, <span class="math inline">\(1 / \sqrt{1 - R_j}\)</span>, is named the <strong>variance inflation factor</strong> (VIF) for variable <span class="math inline">\(j\)</span>. It ranges from <span class="math inline">\(\Inf\)</span> when <span class="math inline">\(x_j\)</span> is completely “explained” (is a linear function of) the other predictors (<span class="math inline">\(R_j^2 = 1\)</span>), to <span class="math inline">\(0\)</span>, when <span class="math inline">\(x_j\)</span> is uncorrelated with the other variables (<span class="math inline">\(R_j^2 = 0\)</span>) The term <span class="math inline">\(\hat{\sigma}^2\)</span> is the standard error of the regression, <span class="math inline">\(\hat{\sigma}^2 = \sum_i \hat{\epsilon}^2 / (n - k - 1)\)</span>.</p>
<p>The variance-covariance matrix of the regression coefficients is <span class="citation">(Fox <a href="#ref-Fox2008a">2008</a>, 199)</span> <span class="math display">\[
\widehat{\cov}(\hat{\vec{\beta}}) = \hat{\sigma}^2 (\mat{X}\T \mat{X})^{-1} .
\]</span></p>
</div>
<div id="single-coefficient" class="section level2">
<h2><span class="header-section-number">7.3</span> Single Coefficient</h2>
<p>Consider these hypothesis about a single <span class="math inline">\(\beta_k\)</span> coefficient: <span class="math display">\[
\begin{aligned}[t]
H_0:&amp; \beta_k = \beta_0 \\
H_a:&amp; \beta_k \neq \beta_0 \\
\end{aligned}
\]</span> The test statistic is, <span class="math display">\[
t = \frac{\hat{\beta}_k - \beta_0}{\widehat{\se}(\hat{\beta}_k)}
\]</span> which is distributed <span class="math inline">\(t_{n - (k + 1)}\)</span>. The <span class="math inline">\(p\)</span> value <span class="math inline">\(p = \Pr(T &lt; t) + \Pr(T &gt; t)\)</span> where <span class="math inline">\(t\)</span> is the test statistic, and <span class="math inline">\(T\)</span> is a random variable distributed Student’s t.</p>
<p>The most common null hypothesis, and the default null hypothesis reported in regression tables and regression software output is that the coefficient is zero, i.e. <span class="math inline">\(\beta_0 = 0\)</span>. This simplifies the test statistic to 0, <span class="math display">\[
t = \frac{\hat\beta_k}{\widehat{\se}(\hat{\beta_k})}
\]</span> Since the critical value for a two-sided p-value with a normal distribution is 1.96, this yields the rule of thumb that <span class="math inline">\(\hat\beta\)</span> is significant at the 5% level if <span class="math inline">\(t &lt; 2\)</span>.</p>
<p>For a one sided hypothesis, such as <span class="math display">\[
\begin{aligned}[t]
H_0:&amp; \beta_k &lt; \beta_0 \\
H_a:&amp; \beta_k \neq \beta_0 \\
\end{aligned}
\]</span> use the same test statistic as above, but halve the <span class="math inline">\(p\)</span>-value since <span class="math inline">\(p = \Pr(T &lt; t)\)</span>.</p>
<div id="confidence-intervals" class="section level3">
<h3><span class="header-section-number">7.3.1</span> Confidence Intervals</h3>
<p>The <span class="math inline">\(1 - \alpha\)</span> confidence interval for a single regression coefficient is <span class="math display">\[
CI(\hat\beta_k, \alpha) = \hat\beta_k \pm t^*_{\alpha / 2} \hat{se}(\hat\beta)
\]</span> where <span class="math inline">\(t^*_{\alpha / 2}\)</span> is the quantile of the Student’s <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n - k - 1\)</span> degrees of freedom, <span class="math inline">\(t^*_{\alpha/2} = t s.t. \Pr(t &lt; T) = 1 - (1 - \alpha / 2)\)</span>.</p>
</div>
</div>
<div id="multiple-coefficients" class="section level2">
<h2><span class="header-section-number">7.4</span> Multiple Coefficients</h2>
<p>We can consider several common confidence intervals and NHST for multiple coefficients.</p>
<div id="f-test" class="section level3">
<h3><span class="header-section-number">7.4.1</span> F-test</h3>
<p>Consider a multiple regression model: <span class="math display">\[
Y_i = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3
\]</span></p>
<p>Consider the null hypothesis is that all the coefficients are equal to zero, and the alternative that at least one coefficient is not zero: <span class="math display">\[
\begin{aligned}[t]
H_0 :&amp; \text{$\beta_1 = 0$ and $\beta_2 = 0$} \\
H_a :&amp; \text{$\beta_1 \neq 0$ or $\beta_2 \neq 0$} 
\end{aligned}
\]</span></p>
<p>To test this hypothesis, compare the fit (residuals) of the model under the null and alternative hypthesis.</p>
<p>Note that these hypothesese are really about a model comparison. Does the model with variables <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> fit better than the model without them. The model without those predictors is called the <em>restricted model</em> and the model with those predictors is the <em>unrestricted model</em>.</p>
<p><em>Unrestricted model (Long model)</em>: The model if <span class="math inline">\(H_a\)</span> is true: <span class="math display">\[
Y_i = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3
\]</span> with estimates <span class="math display">\[
\hat{Y}_i = \beta_0 + \hat\beta_1 X_1 + \hat\beta_2 X_2 + \hat\beta_3 X_3
\]</span> and sum of squared residuals, <span class="math display">\[
SSR_u = \sum_{i = 1}^n (Y_i - \hat{Y}_i)^2
\]</span></p>
<p><em>Restricted model (short model)</em>: The model if the null is true, <span class="math inline">\(\beta_2 = \beta_3 = 0\)</span> <span class="math display">\[
Y_i = \beta_0 + \beta_1 X_1
\]</span> with estimates, <span class="math display">\[
\tilde{Y}_i = \tilde\beta_0 + \tilde\beta_1 X_1
\]</span> and sum of squared residuals, <span class="math display">\[
SSR_r = \sum_{i = 1}^n (Y_i - \tilde{Y}_i)^2
\]</span></p>
<p>Note that the variance of the errors in the unrestricted model has to be smaller than the variances in the restricted model, <span class="math inline">\(SSR_r \leq SSR_u\)</span>. This is because the unrestricted model has all the variables in the restricted model plus some more, so it can’t fit any worse than the restricted model. Remember that variables to a linear model cannot worsen its in-sample fit.</p>
<p>If the null is true, then we would expect that <span class="math inline">\(SSR_r = SSR_u\)</span> apart from sampling variation. The bigger the difference <span class="math inline">\(SSR_r - SSR_u\)</span>, the less plausible the null hypothesis is.</p>
<p><em>F-statistic:</em> The F-statistic is <span class="math display">\[
F = \frac{(SSR_r - SSR_u) / q}{SSR_u / (n - k - 1)} ,
\]</span> where,</p>
<ul>
<li><span class="math inline">\(SSR_r - SSR_u\)</span>: increase in variation explationed (decrease in in-sample fit) when the new variables are removed</li>
<li><span class="math inline">\(q\)</span> : number of restrictions (number of variables hypothesized to be equal to 0 in the null hypothesis)</li>
<li><span class="math inline">\(n - k - 1\)</span>: denominator/unrestricted degrees of freedom.</li>
<li>Intuition <span class="math display">\[
  \frac{\text{increase in prediction error}}{\text{original prediction error}}
  \]</span> where each of these prediction errors is scaled by its degrees of freedom.</li>
</ul>
<p>The sampling distribution of the test statistic, <span class="math inline">\(F\)</span> is the unsurprisingly named <span class="math inline">\(F\)</span>-distribution.<br />
The <a href="https://en.wikipedia.org/wiki/F-distribution">F-distribution</a> is the ratio of two <span class="math inline">\(\chi^2\)</span> (<a href="https://en.wikipedia.org/wiki/Chi-squared_distribution">Chi-squared</a>) distributions.</p>
<p><span class="math display">\[
F = \frac{(SSR_r - SSR_u) / q}{SSR_u / (n - k - 1)} \sim F_{}
\]</span></p>
<p><em>Connection to t-test</em> But isn’t the <span class="math inline">\(t\)</span>-test a special case of a multiple hypothesis test in which only the null hypothesis only has one coefficient set to 0. Yes, yes, it is. The F-statitic for a single restriction is a square of the t-statistic: <span class="math display">\[
F = t^2 = {\left( \frac{\hat{\beta}_1}{\widehat{\se}(\hat{\beta}_1)} \right)}^2
\]</span></p>
<p><em>TODO</em> Simulate this test to show its sampling distribution</p>
</div>
</div>
<div id="confidence-ellipses" class="section level2">
<h2><span class="header-section-number">7.5</span> Confidence Ellipses</h2>
<p>A <em>confidence ellipse</em> is the multivariate generalization of a confidence interval. A <span class="math inline">\(1 - \alpha\)</span>% Confidence intervals computed on repeated i.i.d. samples will contain the vector of true parameter values in <span class="math inline">\(1 - \alpha\)</span> of those samples.</p>
<p>See <span class="citation">Fox (<a href="#ref-Fox2016a">2016</a>)</span> for the derivation of the OLS confidence ellipse. The <strong><a href="https://cran.r-project.org/package=car">car</a></strong> function <a href="https://www.rdocumentation.org/packages/car/topics/confidenceEllipse">car</a> calculates the confidence ellipse. See its help page for examples.</p>
</div>
<div id="linear-hypothesis-tests" class="section level2">
<h2><span class="header-section-number">7.6</span> Linear Hypothesis Tests</h2>
<p>See <span class="citation">Fox (<a href="#ref-Fox2016a">2016</a>)</span> for a discussion. See <a href="https://www.rdocumentation.org/packages/car/topics/LinearHypothesis">car</a> function for an implementation of linear hypothesis testing in R.</p>
</div>
<div id="non-linear-hypothesis-coefficients" class="section level2">
<h2><span class="header-section-number">7.7</span> Non-Linear Hypothesis Coefficients</h2>
<p>Non-linear tests are easiest done with bootstrapping. However, the Delta method can also be used (see <a href="https://www.rdocumentation.org/packages/car/topics/deltaMethod">car</a>.</p>
</div>
<div id="confidence-intervals-for-linear-and-non-linear-functions-of-coefficients" class="section level2">
<h2><span class="header-section-number">7.8</span> Confidence Intervals for Linear and Non-linear functions of Coefficients</h2>
<p>For a single coefficient, a confidence interval for a linear function of <span class="math inline">\(\hat\beta\)</span>, <span class="math display">\[
CI(a + c \hat\beta_k) = a + c CI(\hat\beta_k)
\]</span></p>
<p>For non-linear confidence intervals the easiest way to calculate the confidence intervals is using a bootstrap (see <a href="problems-with-errors.html#bootstrapping">Bootstrapping</a>) or simulation <span class="citation">(King, Tomz, and Wittenberg <a href="#ref-KingTomzWittenberg2000a">2000</a>)</span>.</p>
</div>
<div id="multiple-testing" class="section level2">
<h2><span class="header-section-number">7.9</span> Multiple Testing</h2>
<p>What happens if we run multiple regressions? What do p-values mean in that context?</p>
<p>Simulate data where <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are all simulated from i.i.d. standard normal distributions, <span class="math inline">\(Y_i \sim N(0, 1)\)</span> and <span class="math inline">\(X_{i,j} \sim N(0, 1)\)</span>. This means that <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are not associated.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_reg_nothing &lt;-<span class="st"> </span><span class="cf">function</span>(n, k, <span class="dt">sigma =</span> <span class="dv">1</span>, <span class="dt">.id =</span> <span class="ot">NULL</span>) {
  .data &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n <span class="op">*</span><span class="st"> </span>k, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">matrix</span>(<span class="dt">nrow =</span> n, <span class="dt">ncol =</span> k) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">set_colnames</span>(<span class="kw">str_c</span>(<span class="st">&quot;X&quot;</span>, <span class="kw">seq_len</span>(k))) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">as_tibble</span>()
  .data<span class="op">$</span>y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)
  <span class="co"># Run first regression</span>
  .formula1 &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">str_c</span>(<span class="st">&quot;y&quot;</span>, <span class="st">&quot;~&quot;</span>, <span class="kw">str_c</span>(<span class="st">&quot;X&quot;</span>, <span class="kw">seq_len</span>(k), <span class="dt">collapse =</span> <span class="st">&quot;+&quot;</span>)))
  mod &lt;-<span class="st"> </span><span class="kw">lm</span>(.formula1, <span class="dt">data =</span> .data, <span class="dt">model =</span> <span class="ot">FALSE</span>)
  df &lt;-<span class="st"> </span><span class="kw">tidy</span>(mod)
  df[[<span class="st">&quot;.id&quot;</span>]] &lt;-<span class="st"> </span>.id
  df
}</code></pre></div>
<p>Here is an example with of running one regression:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">1000</span>
k &lt;-<span class="st"> </span><span class="dv">19</span>
results_sim &lt;-<span class="st"> </span><span class="kw">sim_reg_nothing</span>(n, k)</code></pre></div>
<p>How many coefficients are significant at the 5% level?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">alpha &lt;-<span class="st"> </span><span class="fl">0.05</span>
<span class="kw">arrange</span>(results_sim, p.value) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(term, estimate, statistic, p.value) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">head</span>(<span class="dt">n =</span> <span class="dv">20</span>)</code></pre></div>
<pre><code>##           term     estimate   statistic    p.value
## 1           X7 -0.078653549 -2.42975650 0.01528771
## 2           X3  0.074319930  2.31981338 0.02055568
## 3           X2  0.073479210  2.26682346 0.02361823
## 4          X18  0.060116549  1.91166357 0.05621077
## 5          X15 -0.063182860 -1.90934736 0.05650909
## 6           X4  0.055228170  1.70043102 0.08936716
## 7          X16  0.037261288  1.17882830 0.23875268
## 8          X12 -0.031195077 -1.02930126 0.30359210
## 9           X6 -0.030642570 -0.98608544 0.32433457
## 10         X11 -0.031623879 -0.96449749 0.33503450
## 11          X5 -0.027538547 -0.85419387 0.39320633
## 12          X9 -0.017672658 -0.55548574 0.57868924
## 13          X1 -0.016570940 -0.50298851 0.61508537
## 14         X17 -0.013440731 -0.42137369 0.67357463
## 15 (Intercept) -0.011582072 -0.36791210 0.71301823
## 16         X10  0.008910153  0.27966163 0.77979613
## 17          X8  0.008178612  0.26544095 0.79072562
## 18         X13 -0.004758974 -0.14797090 0.88239617
## 19         X19  0.003901157  0.12131962 0.90346275
## 20         X14  0.001949713  0.06442449 0.94864537</code></pre>
<p>Is this surprising? No. Since the null hypothesis is true for all coefficients (<span class="math inline">\(\beta_j = 0\)</span>), a <span class="math inline">\(p\)</span>-value of 5% means that 5% of the tests will be false positives (Type I error).</p>
<p>Let’s confirm that with a larger number of simulations and also use it to calculate some other values. Run 1,024 simulations and save the results to a data frame.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">number_sims &lt;-<span class="st"> </span><span class="dv">1024</span>
sims &lt;-<span class="st"> </span><span class="kw">map_df</span>(<span class="kw">seq_len</span>(number_sims),
               <span class="cf">function</span>(i) {
                 <span class="kw">sim_reg_nothing</span>(n, k, <span class="dt">.id =</span> i)
               })</code></pre></div>
<p>Calculate the number significant at the 5% level in each regression.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n_sig &lt;-
<span class="st">  </span>sims <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(.id) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">num_sig =</span> <span class="kw">sum</span>(p.value <span class="op">&lt;</span><span class="st"> </span>alpha)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">count</span>(num_sig) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">p =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n))</code></pre></div>
<p>Overall, we expect 5% to be significant at the 5 percent level.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sims <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">num_sig =</span> <span class="kw">sum</span>(p.value <span class="op">&lt;</span><span class="st"> </span>alpha), <span class="dt">n =</span> <span class="kw">n</span>()) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">p =</span> num_sig <span class="op">/</span><span class="st"> </span>n)</code></pre></div>
<pre><code>##   num_sig     n          p
## 1     988 20480 0.04824219</code></pre>
<p>What about the distribution of statistically significant coefficients in each regression?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(n_sig, <span class="kw">aes</span>(<span class="dt">x =</span> num_sig, <span class="dt">y =</span> p)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="st">&quot;Number of significant coefs&quot;</span>,
                     <span class="dt">breaks =</span> <span class="kw">unique</span>(n_sig<span class="op">$</span>num_sig)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">&quot;Pr(reg has k signif coef)&quot;</span>)</code></pre></div>
<p><img src="regression-inference_files/figure-html/unnamed-chunk-9-1.svg" width="672" /></p>
<p>What’s the probability that a regression will have no significant coefficients, <span class="math inline">\(1 - (1 - \alpha) ^ {k - 1}\)</span>,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>alpha) <span class="op">^</span><span class="st"> </span>(k <span class="op">+</span><span class="st"> </span><span class="dv">1</span>))</code></pre></div>
<pre><code>## [1] 0.6415141</code></pre>
<p>What’s the take-away? Don’t be too impressed by statistical significance when many tests are run. Note that multiple hypothesis tests occur both within papers and within literatures.</p>
<p><strong>TODO</strong></p>
<ul>
<li>Familywise Error Rate</li>
<li>Familywise Discovery Rate</li>
<li>R function <a href="https://www.rdocumentation.org/packages/stats/topics/p.adj">stats</a> will adjust p-values for multiple testing: Bonferroni, Holm, Hochberg, etc.</li>
</ul>
</div>
<div id="data-snooping" class="section level2">
<h2><span class="header-section-number">7.10</span> Data snooping</h2>
<p>A not-uncommon practice is to run a regression, filter out variables with “insignificant” coefficients, and then run and report a regression with only the smaller number of “significant” variables. Most explicitly, this occurs with <a href="https://en.wikipedia.org/wiki/Stepwise_regression">stepwise regression</a>, the problems of which are well known (when used for inference). However, this can even occur in cases where the hypotheses are not specified in advance and there is no explicit stepwise function used.</p>
<p>To see the issues with this method, let’s consider the worst case scenario, when there is no relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. Suppose <span class="math inline">\(Y_i\)</span> is sampled from a i.i.d. standard normal distributions, <span class="math inline">\(Y_i \sim N(0, 1)\)</span>. Suppose that the design matrix, <span class="math inline">\(\mat{X}\)</span>, consists of 50 variables, each sampled from i.i.d. standard normal distributions, <span class="math inline">\(X_{i,k} \sim N(0, 1)\)</span> for <span class="math inline">\(i \in 1:100\)</span>, <span class="math inline">\(k \in 1:50\)</span>. Given this, the <span class="math inline">\(R^2\)</span> for these regressions should be approximately 0.50. As shown in the previous section, it will not be uncommon to have several “statistically” significant coefficients at the 5 percent level. The <code>sim_datasnoop</code> function simulates data, and runs two regressions:</p>
<ol style="list-style-type: decimal">
<li>Regress <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span></li>
<li>Keep all variables in <span class="math inline">\(X\)</span> with <span class="math inline">\(p &lt; .25\)</span>.</li>
<li>Regress <span class="math inline">\(Y\)</span> on the subset of <span class="math inline">\(X\)</span>, keeping only those variables that were significant in step 2.</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_datasnoop &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="dt">k =</span> <span class="dv">50</span>, <span class="dt">p =</span> <span class="fl">0.10</span>) {
  .data &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n <span class="op">*</span><span class="st"> </span>k, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">matrix</span>(<span class="dt">nrow =</span> n, <span class="dt">ncol =</span> k) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">set_colnames</span>(<span class="kw">str_c</span>(<span class="st">&quot;X&quot;</span>, <span class="kw">seq_len</span>(k))) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">as_tibble</span>()
  .data<span class="op">$</span>y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)
  <span class="co"># Run first regression</span>
  .formula1 &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">str_c</span>(<span class="st">&quot;y&quot;</span>, <span class="st">&quot;~&quot;</span>, <span class="kw">str_c</span>(<span class="st">&quot;X&quot;</span>, <span class="kw">seq_len</span>(k), <span class="dt">collapse =</span> <span class="st">&quot;+&quot;</span>)))
  mod1 &lt;-<span class="st"> </span><span class="kw">lm</span>(.formula1, <span class="dt">data =</span> .data, <span class="dt">model =</span> <span class="ot">FALSE</span>)
  
  <span class="co"># Select model with only significant values (ignoring intercept)</span>
  signif_x &lt;-
<span class="st">    </span><span class="kw">tidy</span>(mod1) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">filter</span>(p.value <span class="op">&lt;</span><span class="st"> </span>p, 
           term <span class="op">!=</span><span class="st"> &quot;(Intercept)&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">    `</span><span class="dt">[[</span><span class="st">`</span>(<span class="st">&quot;term&quot;</span>)
  <span class="cf">if</span> (<span class="kw">length</span>(signif_x <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>)) {
    .formula2 &lt;-<span class="st"> </span><span class="kw">str_c</span>(<span class="kw">str_c</span>(<span class="st">&quot;y&quot;</span>, <span class="st">&quot;~&quot;</span>, <span class="kw">str_c</span>(signif_x, <span class="dt">collapse =</span> <span class="st">&quot;+&quot;</span>)))
    mod2 &lt;-<span class="st"> </span><span class="kw">lm</span>(.formula2, <span class="dt">data =</span> .data, <span class="dt">model =</span> <span class="ot">FALSE</span>)
  } <span class="cf">else</span> {
    mod2 &lt;-<span class="st"> </span><span class="ot">NULL</span>
  }
  <span class="kw">tibble</span>(<span class="dt">mod1 =</span> <span class="kw">list</span>(mod1), <span class="dt">mod2 =</span> <span class="kw">list</span>(mod2))
}</code></pre></div>
<p>Now repeat this simulation 1,024 times, calculate the <span class="math inline">\(R^2\)</span> and number of statistically significant coefficients at <span class="math inline">\(\alpha = .05\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n_sims &lt;-<span class="st"> </span><span class="dv">1024</span>
alpha &lt;-<span class="st"> </span><span class="fl">0.05</span>
sims &lt;-<span class="st"> </span><span class="kw">rerun</span>(n_sims, <span class="kw">sim_datasnoop</span>()) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_rows</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">r2_1 =</span> <span class="kw">map_dbl</span>(mod1, <span class="op">~</span><span class="st"> </span><span class="kw">glance</span>(.x)<span class="op">$</span>r.squared),
    <span class="dt">r2_2 =</span> <span class="kw">map_dbl</span>(mod2, <span class="cf">function</span>(x) <span class="cf">if</span> (<span class="kw">is.null</span>(x)) <span class="ot">NA_real_</span> <span class="cf">else</span> <span class="kw">glance</span>(x)<span class="op">$</span>r.squared),
    <span class="dt">pvalue_1 =</span> <span class="kw">map_dbl</span>(mod1, <span class="op">~</span><span class="st"> </span><span class="kw">glance</span>(.x)<span class="op">$</span>p.value),
    <span class="dt">pvalue_2 =</span> <span class="kw">map_dbl</span>(mod2, <span class="cf">function</span>(x) <span class="cf">if</span> (<span class="kw">is.null</span>(x)) <span class="ot">NA_real_</span> <span class="cf">else</span> <span class="kw">glance</span>(x)<span class="op">$</span>p.value),
    <span class="dt">sig_1 =</span> <span class="kw">map_dbl</span>(mod1,
                      <span class="op">~</span><span class="st"> </span><span class="kw">nrow</span>(<span class="kw">filter</span>(<span class="kw">tidy</span>(.x), term <span class="op">!=</span><span class="st"> &quot;(Intercept)&quot;</span>, p.value <span class="op">&lt;</span><span class="st"> </span>alpha))),
    <span class="dt">sig_2 =</span> <span class="kw">map_dbl</span>(mod2,
                      <span class="cf">function</span>(x) {
                        <span class="cf">if</span> (<span class="kw">is.null</span>(x)) <span class="ot">NA_real_</span>
                        <span class="cf">else</span> <span class="kw">nrow</span>(<span class="kw">filter</span>(<span class="kw">tidy</span>(x), term <span class="op">!=</span><span class="st"> &quot;(Intercept)&quot;</span>, p.value <span class="op">&lt;</span><span class="st"> </span>alpha))
                      })
    )
<span class="kw">select</span>(sims, r2_<span class="dv">1</span>, r2_<span class="dv">2</span>, pvalue_<span class="dv">1</span>, pvalue_<span class="dv">2</span>, sig_<span class="dv">1</span>, sig_<span class="dv">2</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise_all</span>(<span class="kw">funs</span>(<span class="kw">mean</span>(., <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)))</code></pre></div>
<pre><code>## # A tibble: 1 × 6
##        r2_1      r2_2  pvalue_1   pvalue_2   sig_1   sig_2
##       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
## 1 0.5022612 0.1605037 0.5094306 0.03952331 2.47168 2.46802</code></pre>
<p>While the average <span class="math inline">\(R\)</span> squared of the second stage regressions are less, the average <span class="math inline">\(p\)</span>-values of the F-test that all coefficients are zero are much less. The number of statistically significant coefficients in the first and second regressions are approximately the same, which the second regression being slightly</p>
<ul>
<li>What happens if the number of obs, number of variables, and filtering significance level are adjusted?</li>
</ul>
<p>So why are the significance levels of the overall <span class="math inline">\(F\)</span> test incorrect? For a p-value to be correct, it has to have the correct sampling distribution of the observed data. Even though in this simulation we are sampling the data in the first stage from a model that satisfies the assumptions of the F-test, the second stage does not account for the original filtering.</p>
<p>This example is known as <a href="https://en.wikipedia.org/wiki/Freedman%27s_paradox">Freedman’s Paradox</a> <span class="citation">(Freedman <a href="#ref-Freedman1983a">1983</a>)</span>.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Fox2008a">
<p>Fox, John. 2008. <em>Applied Regression Analysis &amp; Generalized Linear Models</em>. 2nd ed. Sage.</p>
</div>
<div id="ref-Fox2016a">
<p>Fox, John. 2016. <em>Applied Regression Analysis &amp; Generalized Linear Models</em>. 3rd ed. Sage.</p>
</div>
<div id="ref-KingTomzWittenberg2000a">
<p>King, Gary, Michael Tomz, and Jason Wittenberg. 2000. “Making the Most of Statistical Analyses: Improving Interpretation and Presentation.” <em>American Journal of Political Science</em> 44 (2). [Midwest Political Science Association, Wiley]: 347–61. <a href="http://www.jstor.org/stable/2669316" class="uri">http://www.jstor.org/stable/2669316</a>.</p>
</div>
<div id="ref-Freedman1983a">
<p>Freedman, David A. 1983. “A Note on Screening Regression Equations.” <em>The American Statistician</em> 37 (2). Informa UK Limited: 152–55. doi:<a href="https://doi.org/10.1080/00031305.1983.10482729">10.1080/00031305.1983.10482729</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="interpreting-coefficients.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="omitted-variable-bias.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jrnold/intro-method-notes/edit/master/regression-inference.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
