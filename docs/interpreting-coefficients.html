<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Data Analysis Notes</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>">
  <meta name="generator" content="bookdown 0.3.6 and GitBook 2.6.7">

  <meta property="og:title" content="Data Analysis Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  <meta name="github-repo" content="jrnold/intro-methods-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Data Analysis Notes" />
  
  <meta name="twitter:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  

<meta name="author" content="Jeffrey B. Arnold">


<meta name="date" content="2017-04-15">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="what-is-regression.html">
<link rel="next" href="regression-inference.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<link href="libs/plotlyjs-1.16.3/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.16.3/plotly-latest.min.js"></script>
<script src="libs/plotly-binding-4.5.6/plotly.js"></script>



<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intro Method Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="part"><span><b>I Exploratory Data Analysis</b></span></li>
<li class="part"><span><b>II Probability</b></span></li>
<li class="part"><span><b>III Inference</b></span></li>
<li class="part"><span><b>IV Linear Regresssion</b></span></li>
<li class="chapter" data-level="2" data-path="bivariate-ols.html"><a href="bivariate-ols.html"><i class="fa fa-check"></i><b>2</b> Bivariate OLS</a><ul>
<li class="chapter" data-level="2.0.1" data-path="bivariate-ols.html"><a href="bivariate-ols.html#ols-is-the-weighted-sum-of-outcomes"><i class="fa fa-check"></i><b>2.0.1</b> OLS is the weighted sum of outcomes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html"><i class="fa fa-check"></i><b>3</b> Goodness of Fit</a><ul>
<li class="chapter" data-level="3.1" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html#root-mean-squared-error-and-standard-error"><i class="fa fa-check"></i><b>3.1</b> Root Mean Squared Error and Standard Error</a></li>
<li class="chapter" data-level="3.2" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html#r-squared"><i class="fa fa-check"></i><b>3.2</b> R squared</a></li>
<li class="chapter" data-level="3.3" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html#maximum-likelihood"><i class="fa fa-check"></i><b>3.3</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="3.4" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html#regression-line"><i class="fa fa-check"></i><b>3.4</b> Regression Line</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>4</b> Multiple Regression</a></li>
<li class="chapter" data-level="5" data-path="what-is-regression.html"><a href="what-is-regression.html"><i class="fa fa-check"></i><b>5</b> What is Regression?</a><ul>
<li class="chapter" data-level="5.1" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-regression-and-what-is-it-used-for"><i class="fa fa-check"></i><b>5.1</b> What is Regression and What is it Used For ?</a></li>
<li class="chapter" data-level="5.2" data-path="what-is-regression.html"><a href="what-is-regression.html#joint-vs.conditional-models"><i class="fa fa-check"></i><b>5.2</b> Joint vs. Conditional models</a></li>
<li class="chapter" data-level="5.3" data-path="what-is-regression.html"><a href="what-is-regression.html#conditional-expectation-function"><i class="fa fa-check"></i><b>5.3</b> Conditional expectation function</a><ul>
<li class="chapter" data-level="5.3.1" data-path="what-is-regression.html"><a href="what-is-regression.html#discrete-covariates"><i class="fa fa-check"></i><b>5.3.1</b> Discrete Covariates</a></li>
<li class="chapter" data-level="5.3.2" data-path="what-is-regression.html"><a href="what-is-regression.html#continuous-covariates"><i class="fa fa-check"></i><b>5.3.2</b> Continuous Covariates</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="interpreting-coefficients.html"><a href="interpreting-coefficients.html"><i class="fa fa-check"></i><b>6</b> Interpreting Coefficients</a><ul>
<li class="chapter" data-level="6.1" data-path="interpreting-coefficients.html"><a href="interpreting-coefficients.html#interactions-and-polynomials"><i class="fa fa-check"></i><b>6.1</b> Interactions and Polynomials</a></li>
<li class="chapter" data-level="6.2" data-path="interpreting-coefficients.html"><a href="interpreting-coefficients.html#average-marginal-effects"><i class="fa fa-check"></i><b>6.2</b> Average Marginal Effects</a></li>
<li class="chapter" data-level="6.3" data-path="interpreting-coefficients.html"><a href="interpreting-coefficients.html#standardized-coefficients"><i class="fa fa-check"></i><b>6.3</b> Standardized Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="regression-inference.html"><a href="regression-inference.html"><i class="fa fa-check"></i><b>7</b> Regression Inference</a><ul>
<li class="chapter" data-level="7.1" data-path="regression-inference.html"><a href="regression-inference.html#prerequisites"><i class="fa fa-check"></i><b>7.1</b> Prerequisites</a></li>
<li class="chapter" data-level="7.2" data-path="regression-inference.html"><a href="regression-inference.html#sampling-distribution-and-standard-errors-of-coefficients"><i class="fa fa-check"></i><b>7.2</b> Sampling Distribution and Standard Errors of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="single-coefficient.html"><a href="single-coefficient.html"><i class="fa fa-check"></i><b>8</b> Single Coefficient</a><ul>
<li class="chapter" data-level="8.1" data-path="single-coefficient.html"><a href="single-coefficient.html#multiple-coefficients"><i class="fa fa-check"></i><b>8.1</b> Multiple Coefficients</a></li>
<li class="chapter" data-level="8.2" data-path="single-coefficient.html"><a href="single-coefficient.html#general-linear-and-non-linear-tests-of-coefficients"><i class="fa fa-check"></i><b>8.2</b> General Linear and Non-linear tests of Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multiple-testing.html"><a href="multiple-testing.html"><i class="fa fa-check"></i><b>9</b> Multiple Testing</a><ul>
<li class="chapter" data-level="9.1" data-path="multiple-testing.html"><a href="multiple-testing.html#multiple-testing-1"><i class="fa fa-check"></i><b>9.1</b> Multiple Testing</a></li>
<li class="chapter" data-level="9.2" data-path="multiple-testing.html"><a href="multiple-testing.html#data-snooping"><i class="fa fa-check"></i><b>9.2</b> Data snooping</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html"><i class="fa fa-check"></i><b>10</b> Omitted Variable Bias</a><ul>
<li class="chapter" data-level="10.1" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#prerequisites-1"><i class="fa fa-check"></i><b>10.1</b> Prerequisites</a></li>
<li class="chapter" data-level="10.2" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#simpsons-paradox"><i class="fa fa-check"></i><b>10.2</b> Simpson’s Paradox</a></li>
<li class="chapter" data-level="10.3" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#omitted-variable-bias-1"><i class="fa fa-check"></i><b>10.3</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="10.4" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#measurement-error"><i class="fa fa-check"></i><b>10.4</b> Measurement Error</a><ul>
<li class="chapter" data-level="10.4.1" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#whats-the-problem"><i class="fa fa-check"></i><b>10.4.1</b> What’s the problem?</a></li>
<li class="chapter" data-level="10.4.2" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#what-to-do-about-it"><i class="fa fa-check"></i><b>10.4.2</b> What to do about it?</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#more-information"><i class="fa fa-check"></i><b>10.5</b> More Information</a><ul>
<li class="chapter" data-level="10.5.1" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#simpsons-paradox-1"><i class="fa fa-check"></i><b>10.5.1</b> Simpson’s Paradox</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="outliers.html"><a href="outliers.html"><i class="fa fa-check"></i><b>11</b> Outliers</a></li>
<li class="chapter" data-level="12" data-path="problems-with-errors.html"><a href="problems-with-errors.html"><i class="fa fa-check"></i><b>12</b> Problems with Errors</a><ul>
<li class="chapter" data-level="12.1" data-path="problems-with-errors.html"><a href="problems-with-errors.html#prerequisites-2"><i class="fa fa-check"></i><b>12.1</b> Prerequisites</a></li>
<li class="chapter" data-level="12.2" data-path="problems-with-errors.html"><a href="problems-with-errors.html#heteroskedasticity"><i class="fa fa-check"></i><b>12.2</b> Heteroskedasticity</a><ul>
<li class="chapter" data-level="12.2.1" data-path="problems-with-errors.html"><a href="problems-with-errors.html#example-duncans-occupation-data"><i class="fa fa-check"></i><b>12.2.1</b> Example: Duncan’s Occupation Data</a></li>
<li class="chapter" data-level="12.2.2" data-path="problems-with-errors.html"><a href="problems-with-errors.html#notes"><i class="fa fa-check"></i><b>12.2.2</b> Notes</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="problems-with-errors.html"><a href="problems-with-errors.html#correlated-errors"><i class="fa fa-check"></i><b>12.3</b> Correlated Errors</a></li>
<li class="chapter" data-level="12.4" data-path="problems-with-errors.html"><a href="problems-with-errors.html#non-normal-errors"><i class="fa fa-check"></i><b>12.4</b> Non-normal Errors</a></li>
<li class="chapter" data-level="12.5" data-path="problems-with-errors.html"><a href="problems-with-errors.html#bootstrapping"><i class="fa fa-check"></i><b>12.5</b> Bootstrapping</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="weighted-regression.html"><a href="weighted-regression.html"><i class="fa fa-check"></i><b>13</b> Weighted Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="weighted-regression.html"><a href="weighted-regression.html#weighted-least-squares-wls"><i class="fa fa-check"></i><b>13.1</b> Weighted Least Squares (WLS)</a></li>
<li class="chapter" data-level="13.2" data-path="weighted-regression.html"><a href="weighted-regression.html#when-should-you-use-wls"><i class="fa fa-check"></i><b>13.2</b> When should you use WLS?</a></li>
<li class="chapter" data-level="13.3" data-path="weighted-regression.html"><a href="weighted-regression.html#correcting-for-known-heteroskedasticity"><i class="fa fa-check"></i><b>13.3</b> Correcting for Known Heteroskedasticity</a></li>
<li class="chapter" data-level="13.4" data-path="weighted-regression.html"><a href="weighted-regression.html#sampling-weights"><i class="fa fa-check"></i><b>13.4</b> Sampling Weights</a></li>
<li class="chapter" data-level="13.5" data-path="weighted-regression.html"><a href="weighted-regression.html#references"><i class="fa fa-check"></i><b>13.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="discrete-outcome-variables.html"><a href="discrete-outcome-variables.html"><i class="fa fa-check"></i><b>14</b> Discrete Outcome Variables</a><ul>
<li class="chapter" data-level="14.1" data-path="discrete-outcome-variables.html"><a href="discrete-outcome-variables.html#linear-probability-model"><i class="fa fa-check"></i><b>14.1</b> Linear Probability Model</a></li>
<li class="chapter" data-level="14.2" data-path="discrete-outcome-variables.html"><a href="discrete-outcome-variables.html#logit-model"><i class="fa fa-check"></i><b>14.2</b> Logit Model</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="robust-regression.html"><a href="robust-regression.html"><i class="fa fa-check"></i><b>15</b> Robust Regression</a><ul>
<li class="chapter" data-level="15.1" data-path="robust-regression.html"><a href="robust-regression.html#prerequites"><i class="fa fa-check"></i><b>15.1</b> Prerequites</a></li>
<li class="chapter" data-level="15.2" data-path="robust-regression.html"><a href="robust-regression.html#examples"><i class="fa fa-check"></i><b>15.2</b> Examples</a></li>
<li class="chapter" data-level="15.3" data-path="robust-regression.html"><a href="robust-regression.html#notes-1"><i class="fa fa-check"></i><b>15.3</b> Notes</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html"><i class="fa fa-check"></i><b>16</b> Prediction and Model Comparison</a><ul>
<li class="chapter" data-level="16.1" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#prerequisites-3"><i class="fa fa-check"></i><b>16.1</b> Prerequisites</a></li>
<li class="chapter" data-level="16.2" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#measures-of-prediction"><i class="fa fa-check"></i><b>16.2</b> Measures of Prediction</a></li>
<li class="chapter" data-level="16.3" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#model-comparison"><i class="fa fa-check"></i><b>16.3</b> Model Comparison</a></li>
<li class="chapter" data-level="16.4" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#example-predicting-the-price-of-wine"><i class="fa fa-check"></i><b>16.4</b> Example: Predicting the Price of Wine</a></li>
<li class="chapter" data-level="16.5" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#cross-validation"><i class="fa fa-check"></i><b>16.5</b> Cross-Validation</a></li>
<li class="chapter" data-level="16.6" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#out-of-sample-error"><i class="fa fa-check"></i><b>16.6</b> Out of Sample Error</a><ul>
<li class="chapter" data-level="16.6.1" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#held-out-data"><i class="fa fa-check"></i><b>16.6.1</b> Held-out data</a></li>
<li class="chapter" data-level="16.6.2" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>16.6.2</b> Leave-One-Out Cross-Validation</a></li>
<li class="chapter" data-level="16.6.3" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>16.6.3</b> k-fold Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#analytic-covariance-methods"><i class="fa fa-check"></i><b>16.7</b> Analytic Covariance Methods</a></li>
<li class="chapter" data-level="16.8" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#further-resources"><i class="fa fa-check"></i><b>16.8</b> Further Resources</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="miscellaneous-regression-stuff.html"><a href="miscellaneous-regression-stuff.html"><i class="fa fa-check"></i><b>17</b> Miscellaneous Regression Stuff</a><ul>
<li class="chapter" data-level="17.1" data-path="miscellaneous-regression-stuff.html"><a href="miscellaneous-regression-stuff.html#anscombe-quartet"><i class="fa fa-check"></i><b>17.1</b> Anscombe quartet</a></li>
<li class="chapter" data-level="17.2" data-path="miscellaneous-regression-stuff.html"><a href="miscellaneous-regression-stuff.html#correlation-plots"><i class="fa fa-check"></i><b>17.2</b> Correlation Plots</a></li>
</ul></li>
<li class="part"><span><b>V Programming</b></span></li>
<li class="chapter" data-level="18" data-path="rs-forumula-syntax.html"><a href="rs-forumula-syntax.html"><i class="fa fa-check"></i><b>18</b> R’s Forumula Syntax</a><ul>
<li class="chapter" data-level="18.1" data-path="rs-forumula-syntax.html"><a href="rs-forumula-syntax.html#setup"><i class="fa fa-check"></i><b>18.1</b> Setup</a></li>
<li class="chapter" data-level="18.2" data-path="rs-forumula-syntax.html"><a href="rs-forumula-syntax.html#introduction-to-formula-objects"><i class="fa fa-check"></i><b>18.2</b> Introduction to Formula Objects</a></li>
<li class="chapter" data-level="18.3" data-path="rs-forumula-syntax.html"><a href="rs-forumula-syntax.html#programming-with-formulas"><i class="fa fa-check"></i><b>18.3</b> Programming with Formulas</a></li>
</ul></li>
<li class="part"><span><b>VI Examples</b></span></li>
<li class="chapter" data-level="19" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html"><i class="fa fa-check"></i><b>19</b> Duncan Occupational Prestige</a><ul>
<li class="chapter" data-level="19.1" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#setup-1"><i class="fa fa-check"></i><b>19.1</b> Setup</a></li>
<li class="chapter" data-level="19.2" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#coefficients-standard-errors"><i class="fa fa-check"></i><b>19.2</b> Coefficients, Standard errors</a></li>
<li class="chapter" data-level="19.3" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#residuals-fitted-values"><i class="fa fa-check"></i><b>19.3</b> Residuals, Fitted Values,</a></li>
<li class="chapter" data-level="19.4" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#broom"><i class="fa fa-check"></i><b>19.4</b> Broom</a></li>
<li class="chapter" data-level="19.5" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#plotting-fitted-regression-results"><i class="fa fa-check"></i><b>19.5</b> Plotting Fitted Regression Results</a></li>
</ul></li>
<li class="part"><span><b>VII Presentation</b></span></li>
<li class="chapter" data-level="20" data-path="formatting-tables.html"><a href="formatting-tables.html"><i class="fa fa-check"></i><b>20</b> Formatting Tables</a><ul>
<li class="chapter" data-level="20.1" data-path="formatting-tables.html"><a href="formatting-tables.html#overview-of-packages"><i class="fa fa-check"></i><b>20.1</b> Overview of Packages</a></li>
<li class="chapter" data-level="20.2" data-path="formatting-tables.html"><a href="formatting-tables.html#summary-statistic-table-example"><i class="fa fa-check"></i><b>20.2</b> Summary Statistic Table Example</a></li>
<li class="chapter" data-level="20.3" data-path="formatting-tables.html"><a href="formatting-tables.html#regression-table-example"><i class="fa fa-check"></i><b>20.3</b> Regression Table Example</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="reproducible-research.html"><a href="reproducible-research.html"><i class="fa fa-check"></i><b>21</b> Reproducible Research</a></li>
<li class="chapter" data-level="22" data-path="writing-resources.html"><a href="writing-resources.html"><i class="fa fa-check"></i><b>22</b> Writing Resources</a><ul>
<li class="chapter" data-level="22.1" data-path="writing-resources.html"><a href="writing-resources.html#writing-and-organizing-papers"><i class="fa fa-check"></i><b>22.1</b> Writing and Organizing Papers</a></li>
<li class="chapter" data-level="22.2" data-path="writing-resources.html"><a href="writing-resources.html#finding-research-ideas"><i class="fa fa-check"></i><b>22.2</b> Finding Research Ideas</a></li>
<li class="chapter" data-level="22.3" data-path="writing-resources.html"><a href="writing-resources.html#replications"><i class="fa fa-check"></i><b>22.3</b> Replications</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="multivariate-normal-distribution.html"><a href="multivariate-normal-distribution.html"><i class="fa fa-check"></i><b>A</b> Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
\[
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\RMSE}{RMSE}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\T}{'}

\newcommand{\distr}[1]{\mathcal{#1}}
\newcommand{\dnorm}{\distr{N}}
\newcommand{\dmvnorm}[1]{\distr{N}_{#1}}
\newcommand{\dt}[1]{\distr{T}_{#1}}

\newcommand{\cia}{\perp\!\!\!\perp}
\DeclareMathOperator*{\plim}{plim}
\]
<div id="interpreting-coefficients" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Interpreting Coefficients</h1>
<p>That the coefficients are the marginal effects of each predictor makes linear regression particularly easy to interpret. However, this interpretation of predictors becomes more complicated once a variable is included in multiple terms through interactions or nonlinear functions, such as polynomials.</p>
<p>Consider the regression, <span class="math display">\[
Y_i = \beta_0 + \beta_1 X + \beta_2 Z + \varepsilon
\]</span> The regression coefficient <span class="math inline">\(\beta_1\)</span> it the change in the expected value of <span class="math inline">\(Y\)</span> associated with a one-unit change in <span class="math inline">\(X\)</span> holding <span class="math inline">\(Z\)</span> constant, <span class="math display">\[
\begin{aligned}[t]
E(Y | X = x, Z = z) - E(Y | X = x + 1, Z = z) &amp;= (\beta_0 + \beta_1 X + \beta_2 z) - (\beta_0 + \beta_1 (x + 1) + \beta_2 z) \\
&amp;= \beta_1 x - \beta_1 (x + 1) \\
&amp;= \beta_1 (x - x + 1) \\
&amp;= \beta_1
\end{aligned}
\]</span></p>
<p>More formally, the coefficient <span class="math inline">\(\beta_k\)</span> is the partial derivative of <span class="math inline">\(E(Y | X)\)</span> with respect to <span class="math inline">\(X_k\)</span>, <span class="math display">\[
\begin{aligned}[t]
\frac{\partial\,E(Y | X)}{\partial\,X_k} =  \frac{\partial}{\partial\,X_k} \left( \beta_0 + \sum_{k = 1}^K \beta_k X_k) \right) = \beta_k
\end{aligned}
\]</span></p>
<p>Implications:</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(X\)</span> is multiplied by a constant scalar <span class="math inline">\(a\)</span>, <span class="math display">\[
E(Y | X) = \tilde{\beta}_0 + \tilde{\beta}_1 a X = \beta_0 + (a \beta_1) X .
\]</span></li>
<li>If <span class="math inline">\(X_k\)</span> has a scalar <span class="math inline">\(a\)</span> added to it, <span class="math display">\[
E(Y | X) = \tilde{\beta}_0 + \tilde{\beta}_1 (X + a) = (\beta_0 + \tilde{\beta}_1 a) + \tilde{\beta}_1 X
\]</span> Thus, <span class="math inline">\(\tilde{\beta}_0 = (\beta_0 + \beta_1 a)\)</span> and <span class="math inline">\(\tilde{\beta}_1 = \beta_1\)</span>.</li>
</ol>
<p>Consider the regression model <span class="math display">\[
\vec{Y} = \vec{\beta} \mat{X} + \vec{\epsilon}
\]</span></p>
<p>Rather than staring by asking what do the regression coefficients, <span class="math inline">\(\vec{\beta}\)</span>, mean, we should start by asking what we want to estimate (i.e. the estimand) and then figure out how to extract that from the regression model. Let’s start with what it that we want to calculate. We want to calculate the “marginal effect” of changing the <span class="math inline">\(j\)</span>th predictors while holding other predictors constant.</p>
<p>In particular, one common estimand is the predicted change in the expected value of <span class="math inline">\(Y\)</span> from a change in the <span class="math inline">\(j\)</span>th predictor variable while holding the other predictors constant. The regression model is a model of the expected value of <span class="math inline">\(Y\)</span> as a function of <span class="math inline">\(\mat{X}\)</span>, <span class="math display">\[
\hat{\vec{y}} = \E(Y) = \hat{\vec{beta}} \mat{X}
\]</span> For a continuous variable, <span class="math inline">\(\vec{x}_j\)</span>, this is called the “marginal effect” and it is the partial derivative of the regression line with respect to <span class="math inline">\(\vec{x}_j\)</span>, <span class="math display">\[
ME_{i,j} =  \frac{\partial \E( Y_i | x_{i,j}, x_{i,-j})}{\partial x_{i,j}}
\]</span> For a discrete change in <span class="math inline">\(x_j\)</span>, this is called the “partial effect” or “first difference”, and is simply the difference of predicted values, <span class="math display">\[
ME_{i,j} =  \E(Y_i | x_{i,j}, x_{i,-j}) - \E(Y_i | x_{i,j} + \Delta x_{i,j}, x_{i,-j})
\]</span></p>
<p>Now consider the linear regression with two predictors for a change in <span class="math inline">\(x_1\)</span>, <span class="math display">\[
\begin{aligned}[t]
ME_j &amp;=  E(y | x_1, \tilde{x}_2) - E(y | x_1 + \Delta x_1, \tilde{x}_2)
\end{aligned}
\]</span> Since the linear regression equation is <span class="math inline">\(E(y | x)\)</span>, this simplifies to <span class="math display">\[
\begin{aligned}[t]
ME_j &amp;=  (\beta_0 + \beta_1 x_1 + \tilde{x}_2) - (\beta_0 + \beta_1 (x_1 + \Delta x_1) \tilde{x}_2) \\
&amp;= \beta_1 \Delta x_1
\end{aligned}
\]</span> or as <span class="math inline">\(\Delta x_1 \to 0\)</span>, this simplifies to the coefficient itself. <span class="math display">\[
\begin{aligned}[t]
ME_j &amp;=  \frac{\partial E(y | x_1, x_2)}{\partial x_1} \\
\frac{\partial (\beta_0 + \beta_1 x_1 + \tilde{x}_2)}{\partial x_1}
&amp;= \beta_1
\end{aligned}
\]</span></p>
<p>All of the previous equations were at the population level. The sample estimate for the marginal effect is <span class="math display">\[
\widehat{ME}_j = \hat{\beta}_1
\]</span></p>
<p>So, for a linear regression, the marginal effect of <span class="math inline">\(x_j\)</span>, defined as the change in the expected value of <span class="math inline">\(y\)</span> for a small a unit of <span class="math inline">\(j\)</span></p>
<p>The equation presented above is not <strong>causal</strong>, it is simply a function derived from the population or estimated equation.</p>
<p>If population equation is not the as the linear regression, <span class="math inline">\(\hat{\beta_j}\)</span> can still be viewed as an estimator of <span class="math inline">\(ME_j\)</span>. In OLS, the <span class="math inline">\(ME_j\)</span> is weighted by observations with the most variation in <span class="math inline">\(x_j\)</span>, after accounting for the parts of <span class="math inline">\(x_j\)</span> and <span class="math inline">\(y\)</span> predicted by the other predictors. See the discussion <span class="citation">Angrist and Pischke (<a href="#ref-AngristPischke2009a">2009</a>)</span> and <span class="citation">Aronow and Samii (<a href="#ref-AronowSamii2015a">2015</a>)</span>.</p>
<p>For regressions other than OLS, the coefficients are not the <span class="math inline">\(ME_j\)</span>. It is a luxury that the coefficients happen to have a nice interpretation in OLS. In most other regressions, the coefficients are not directly useful. This is yet another reason to avoid the mindless presentation of tables and star-worshiping. The researcher should focus on inference about the research quantity of interest, whether or not that happens to be conveniently provided as a parameter of the model that was estimated.</p>
<div id="interactions-and-polynomials" class="section level2">
<h2><span class="header-section-number">6.1</span> Interactions and Polynomials</h2>
<p>Even for OLS, if <span class="math inline">\(x_j\)</span> is included as part of a function, e.g. a polynomial or an interaction, then its coefficient cannot be interpreted as the marginal effect. Suppose that the regression equation is <span class="math display">\[
\vec{y} = \vec{\beta}_0 + \vec{\beta}_1 x_1 + \vec{\beta}_2 x_1^2 + \vec{\beta}_3 x_2,
\]</span> then the marginal effect of <span class="math inline">\(x_1\)</span> is, <span class="math display">\[
\begin{aligned}[t]
ME_j &amp;=  \frac{\partial E(y | x_1, x_2)}{\partial x_1} \\
&amp;= \frac{\partial (\beta_0 + \beta_1 x_1 + \beta_1 x_1^2 +  \beta_3 \tilde{x}_2)}{\partial x_1} \\
&amp;= \beta_1 + 2 \beta_2 x_1
\end{aligned}
\]</span> Note that the marginal effect of <span class="math inline">\(x_1\)</span> is <strong>not</strong>, <span class="math inline">\(\beta_1\)</span>. That would require a change in <span class="math inline">\(x_1\)</span> while holding <span class="math inline">\(x_1 ^ 2\)</span> constant, which is a logical impossibility. Instead, the marginal effect of <span class="math inline">\(x_1\)</span> depends on the value of <span class="math inline">\(x_2\)</span> at which it is evaluated, and, thus, observations will have different marginal effects.</p>
<p>Similarly, if there is an interaction between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, the coefficient of a predictor is not its marginal effect. For example, in <span class="math display">\[
y = \vec{\beta}_0 + \vec{\beta}_1 x_1 + \vec{\beta}_2 x_1 + \vec{\beta}_3 x_1 x_2
\]</span> the marginal effect of <span class="math inline">\(x_1\)</span> is <span class="math display">\[
\begin{aligned}[t]
ME_j &amp;=  \frac{\partial E(y | x_1, x_2)}{\partial x_1} \\
&amp;= \frac{\partial (\beta_0 + \beta_1 x_1 + \beta_1 x_1^2 +  \beta_2 \tilde{x}_2)}{\partial x_1} \\
&amp;= \beta_1 + \beta_3 x_2
\end{aligned}
\]</span> Now the marginal effect of <span class="math inline">\(x_1\)</span> is a function of another variable <span class="math inline">\(x_2\)</span>.</p>
</div>
<div id="average-marginal-effects" class="section level2">
<h2><span class="header-section-number">6.2</span> Average Marginal Effects</h2>
<p>For marginal effects that are functions of the data, there are multiple ways to calculate them. They include,</p>
<ul>
<li>AME: Average Marginal Effect. Average the marginal effects at each observed <span class="math inline">\(x\)</span>.</li>
<li>MEM: Marginal Effect at the mean. Calculate the marginal effect with all observations at their means or other central values.</li>
<li>MER: Marginal Effect at a representative value. Similar to MEM but with another meaningful value.</li>
</ul>
<p>Of these, the AME is the preferred one; marginal effects should be calculated for all observations, and then averaged <span class="citation">(Hanmer and Kalkan <a href="#ref-HanmerKalkan2012a">2012</a>)</span>.</p>
<!-- When it is discrete change in $x$, it is called a partial effect (APE) or a first difference. -->
<!-- The difference in the expected value of y, given a change in $x_j$ from $x^*$ to $x^* + \Delta$ is $\beta_j \Delta$, and the standard error can be approximated  by [https://en.wikipedia.org/wiki/Delta_method](https://en.wikipedia.org/wiki/Delta_method), -->
<!-- $$ -->
<!-- \se(\hat{\beta}_j \Delta x) = \sqrt{\Var\hat{\beta_j} (\Delta x)^2 } = \se\hat{\beta_j} \Delta x. -->
<!-- $$ -->
<!-- The Delta method can be used to analytically derive approximations of the standard errors for other nonlinear functions and interaction in regression, but it scales poorly, and it is often easier to use bootstrapping or software than calculate it by hand. See the [margins](https://github.com/leeper/margins) package. -->
<p>When it is discrete change in <span class="math inline">\(x\)</span>, it is called a partial effect (APE) or a first difference. The difference in the expected value of y, given a change in <span class="math inline">\(x_j\)</span> from <span class="math inline">\(x^*\)</span> to <span class="math inline">\(x^* + \Delta\)</span> is <span class="math inline">\(\beta_j \Delta\)</span>, and the standard error can be calculated analytically by the <a href="https://en.wikipedia.org/wiki/Delta_method">Delta method</a>, <span class="math display">\[
\se(\hat{\beta}_j \Delta x_j) = \sqrt{\Var\hat{\beta}_j (\Delta x_j)^2} = \se\hat{\beta}_j \Delta x_j.
\]</span> The Delta method can be used to analytically derive approximations of the standard errors for other nonlinear functions and interaction in regression, but it scales poorly, and it is often easier to use bootstrapping or software than calculate it by hand. See the <a href="https://github.com/leeper/margins">margins</a> package.</p>
</div>
<div id="standardized-coefficients" class="section level2">
<h2><span class="header-section-number">6.3</span> Standardized Coefficients</h2>
<p>A standardized coefficient is the coefficient on <span class="math inline">\(X\)</span>, when <span class="math inline">\(X\)</span> is standardized so that <span class="math inline">\(\mean(X) = 0\)</span> and <span class="math inline">\(\Var(X) = 1\)</span>. In that case, <span class="math inline">\(\beta_1\)</span> is the change in <span class="math inline">\(\E(Y)\)</span> associated with a one standard deviation change in <span class="math inline">\(X\)</span>.</p>
<p>Additionally, if all predictors are set so that <span class="math inline">\(\mean(X) = 0\)</span>, <span class="math inline">\(\beta_0\)</span> is the expected value of <span class="math inline">\(Y\)</span> when all <span class="math inline">\(X\)</span> are at their means. However, if any variables appear in multiple terms, then the standardized coefficients are not particularly useful.</p>
<p>Standardized coefficients are generally not used in political science. (King How Not to Lie with Statistics, p. 669) More often, the effects of variables are compared by the first difference between the value of the variable at the mean, and a one standard deviation change. While, this is equivalent to the standardized coefficient</p>
<p>Note, that standardizing variables can help computationally in some cases. In OLS, there is a closed-form solution, so iterative optimization algorithms are not needed in to find the best parameters. However, in more complicated models which require iterative optimization, standardizing variables can often improve the performance of the optimization. Thus standardizing variables before analysis is common in machine learning. However, the purpose is for ease of computation, not for ease of interpretation.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-AngristPischke2009a">
<p>Angrist, Joshua D., and Jörn-Steffen Pischke. 2009. <em>Mostly Harmless Econometrics: An Empiricist’s Companion</em>. Pr.</p>
</div>
<div id="ref-AronowSamii2015a">
<p>Aronow, Peter M., and Cyrus Samii. 2015. “Does Regression Produce Representative Estimates of Causal Effects?” <em>American Journal of Political Science</em> 60 (1). Wiley-Blackwell: 250–67. doi:<a href="https://doi.org/10.1111/ajps.12185">10.1111/ajps.12185</a>.</p>
</div>
<div id="ref-HanmerKalkan2012a">
<p>Hanmer, Michael J., and Kerem Ozan Kalkan. 2012. “Behind the Curve: Clarifying the Best Approach to Calculating Predicted Probabilities and Marginal Effects from Limited Dependent Variable Models.” <em>American Journal of Political Science</em> 57 (1). Wiley-Blackwell: 263–77. doi:<a href="https://doi.org/10.1111/j.1540-5907.2012.00602.x">10.1111/j.1540-5907.2012.00602.x</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="what-is-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression-inference.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jrnold/intro-method-notes/edit/master/marginaleffects.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
