<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Data Analysis Notes</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Data Analysis Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  <meta name="github-repo" content="jrnold/intro-methods-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Data Analysis Notes" />
  
  <meta name="twitter:description" content="<p>These are notes associated with the course, POLS/CS&amp;SS 503: Advanced Quantitative Political Methodology at the University of Washington.</p>" />
  

<meta name="author" content="Jeffrey B. Arnold">


<meta name="date" content="2017-04-21">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="omitted-variable-bias.html">
<link rel="next" href="problems-with-errors.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<link href="libs/plotlyjs-1.16.3/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.16.3/plotly-latest.min.js"></script>
<script src="libs/plotly-binding-4.5.6/plotly.js"></script>



<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>

\[
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\RMSE}{RMSE}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\T}{'}

\newcommand{\distr}[1]{\mathcal{#1}}
\newcommand{\dnorm}{\distr{N}}
\newcommand{\dmvnorm}[1]{\distr{N}_{#1}}
\newcommand{\dt}[1]{\distr{T}_{#1}}

\newcommand{\cia}{\perp\!\!\!\perp}
\DeclareMathOperator*{\plim}{plim}
\]

  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intro Method Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="part"><span><b>I Exploratory Data Analysis</b></span></li>
<li class="part"><span><b>II Probability</b></span></li>
<li class="part"><span><b>III Inference</b></span></li>
<li class="part"><span><b>IV Linear Regresssion</b></span></li>
<li class="chapter" data-level="2" data-path="bivariate-ols.html"><a href="bivariate-ols.html"><i class="fa fa-check"></i><b>2</b> Bivariate OLS</a><ul>
<li class="chapter" data-level="2.0.1" data-path="bivariate-ols.html"><a href="bivariate-ols.html#ols-is-the-weighted-sum-of-outcomes"><i class="fa fa-check"></i><b>2.0.1</b> OLS is the weighted sum of outcomes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html"><i class="fa fa-check"></i><b>3</b> Goodness of Fit</a><ul>
<li class="chapter" data-level="3.1" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html#root-mean-squared-error-and-standard-error"><i class="fa fa-check"></i><b>3.1</b> Root Mean Squared Error and Standard Error</a></li>
<li class="chapter" data-level="3.2" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html#r-squared"><i class="fa fa-check"></i><b>3.2</b> R squared</a></li>
<li class="chapter" data-level="3.3" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html#maximum-likelihood"><i class="fa fa-check"></i><b>3.3</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="3.4" data-path="goodness-of-fit.html"><a href="goodness-of-fit.html#regression-line"><i class="fa fa-check"></i><b>3.4</b> Regression Line</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>4</b> Multiple Regression</a></li>
<li class="chapter" data-level="5" data-path="what-is-regression.html"><a href="what-is-regression.html"><i class="fa fa-check"></i><b>5</b> What is Regression?</a><ul>
<li class="chapter" data-level="5.1" data-path="what-is-regression.html"><a href="what-is-regression.html#what-is-regression-and-what-is-it-used-for"><i class="fa fa-check"></i><b>5.1</b> What is Regression and What is it Used For ?</a></li>
<li class="chapter" data-level="5.2" data-path="what-is-regression.html"><a href="what-is-regression.html#joint-vs.conditional-models"><i class="fa fa-check"></i><b>5.2</b> Joint vs. Conditional models</a></li>
<li class="chapter" data-level="5.3" data-path="what-is-regression.html"><a href="what-is-regression.html#conditional-expectation-function"><i class="fa fa-check"></i><b>5.3</b> Conditional expectation function</a><ul>
<li class="chapter" data-level="5.3.1" data-path="what-is-regression.html"><a href="what-is-regression.html#discrete-covariates"><i class="fa fa-check"></i><b>5.3.1</b> Discrete Covariates</a></li>
<li class="chapter" data-level="5.3.2" data-path="what-is-regression.html"><a href="what-is-regression.html#continuous-covariates"><i class="fa fa-check"></i><b>5.3.2</b> Continuous Covariates</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="interpreting-coefficients.html"><a href="interpreting-coefficients.html"><i class="fa fa-check"></i><b>6</b> Interpreting Coefficients</a><ul>
<li class="chapter" data-level="6.1" data-path="interpreting-coefficients.html"><a href="interpreting-coefficients.html#interactions-and-polynomials"><i class="fa fa-check"></i><b>6.1</b> Interactions and Polynomials</a></li>
<li class="chapter" data-level="6.2" data-path="interpreting-coefficients.html"><a href="interpreting-coefficients.html#average-marginal-effects"><i class="fa fa-check"></i><b>6.2</b> Average Marginal Effects</a></li>
<li class="chapter" data-level="6.3" data-path="interpreting-coefficients.html"><a href="interpreting-coefficients.html#standardized-coefficients"><i class="fa fa-check"></i><b>6.3</b> Standardized Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="regression-inference.html"><a href="regression-inference.html"><i class="fa fa-check"></i><b>7</b> Regression Inference</a><ul>
<li class="chapter" data-level="7.1" data-path="regression-inference.html"><a href="regression-inference.html#prerequisites"><i class="fa fa-check"></i><b>7.1</b> Prerequisites</a></li>
<li class="chapter" data-level="7.2" data-path="regression-inference.html"><a href="regression-inference.html#sampling-distribution-and-standard-errors-of-coefficients"><i class="fa fa-check"></i><b>7.2</b> Sampling Distribution and Standard Errors of Coefficients</a></li>
<li class="chapter" data-level="7.3" data-path="regression-inference.html"><a href="regression-inference.html#single-coefficient"><i class="fa fa-check"></i><b>7.3</b> Single Coefficient</a><ul>
<li class="chapter" data-level="7.3.1" data-path="regression-inference.html"><a href="regression-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>7.3.1</b> Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="regression-inference.html"><a href="regression-inference.html#multiple-coefficients"><i class="fa fa-check"></i><b>7.4</b> Multiple Coefficients</a><ul>
<li class="chapter" data-level="7.4.1" data-path="regression-inference.html"><a href="regression-inference.html#f-test"><i class="fa fa-check"></i><b>7.4.1</b> F-test</a></li>
<li class="chapter" data-level="7.4.2" data-path="regression-inference.html"><a href="regression-inference.html#confidence-regions"><i class="fa fa-check"></i><b>7.4.2</b> Confidence Regions</a></li>
<li class="chapter" data-level="7.4.3" data-path="regression-inference.html"><a href="regression-inference.html#linear-hypothesis-tests"><i class="fa fa-check"></i><b>7.4.3</b> Linear Hypothesis Tests</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="regression-inference.html"><a href="regression-inference.html#linear-and-non-linear-confidence-intervals"><i class="fa fa-check"></i><b>7.5</b> Linear and Non-Linear Confidence Intervals</a></li>
<li class="chapter" data-level="7.6" data-path="regression-inference.html"><a href="regression-inference.html#multiple-testing"><i class="fa fa-check"></i><b>7.6</b> Multiple Testing</a></li>
<li class="chapter" data-level="7.7" data-path="regression-inference.html"><a href="regression-inference.html#data-snooping"><i class="fa fa-check"></i><b>7.7</b> Data snooping</a></li>
<li class="chapter" data-level="7.8" data-path="regression-inference.html"><a href="regression-inference.html#power"><i class="fa fa-check"></i><b>7.8</b> Power</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html"><i class="fa fa-check"></i><b>8</b> Omitted Variable Bias</a><ul>
<li class="chapter" data-level="8.1" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#prerequisites-1"><i class="fa fa-check"></i><b>8.1</b> Prerequisites</a></li>
<li class="chapter" data-level="8.2" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#simpsons-paradox"><i class="fa fa-check"></i><b>8.2</b> Simpson’s Paradox</a></li>
<li class="chapter" data-level="8.3" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#omitted-variable-bias-1"><i class="fa fa-check"></i><b>8.3</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="8.4" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#measurement-error"><i class="fa fa-check"></i><b>8.4</b> Measurement Error</a><ul>
<li class="chapter" data-level="8.4.1" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#whats-the-problem"><i class="fa fa-check"></i><b>8.4.1</b> What’s the problem?</a></li>
<li class="chapter" data-level="8.4.2" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#what-to-do-about-it"><i class="fa fa-check"></i><b>8.4.2</b> What to do about it?</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#more-information"><i class="fa fa-check"></i><b>8.5</b> More Information</a><ul>
<li class="chapter" data-level="8.5.1" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html#simpsons-paradox-1"><i class="fa fa-check"></i><b>8.5.1</b> Simpson’s Paradox</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="outliers.html"><a href="outliers.html"><i class="fa fa-check"></i><b>9</b> Outliers</a><ul>
<li class="chapter" data-level="9.1" data-path="outliers.html"><a href="outliers.html#iver-and-soskice-data"><i class="fa fa-check"></i><b>9.1</b> Iver and Soskice Data</a></li>
<li class="chapter" data-level="9.2" data-path="outliers.html"><a href="outliers.html#influential-observations"><i class="fa fa-check"></i><b>9.2</b> Influential Observations</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="problems-with-errors.html"><a href="problems-with-errors.html"><i class="fa fa-check"></i><b>10</b> Problems with Errors</a><ul>
<li class="chapter" data-level="10.1" data-path="problems-with-errors.html"><a href="problems-with-errors.html#prerequisites-2"><i class="fa fa-check"></i><b>10.1</b> Prerequisites</a></li>
<li class="chapter" data-level="10.2" data-path="problems-with-errors.html"><a href="problems-with-errors.html#heteroskedasticity"><i class="fa fa-check"></i><b>10.2</b> Heteroskedasticity</a><ul>
<li class="chapter" data-level="10.2.1" data-path="problems-with-errors.html"><a href="problems-with-errors.html#example-duncans-occupation-data"><i class="fa fa-check"></i><b>10.2.1</b> Example: Duncan’s Occupation Data</a></li>
<li class="chapter" data-level="10.2.2" data-path="problems-with-errors.html"><a href="problems-with-errors.html#notes"><i class="fa fa-check"></i><b>10.2.2</b> Notes</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="problems-with-errors.html"><a href="problems-with-errors.html#non-normal-errors"><i class="fa fa-check"></i><b>10.3</b> Non-normal Errors</a></li>
<li class="chapter" data-level="10.4" data-path="problems-with-errors.html"><a href="problems-with-errors.html#clustered-standard-errors"><i class="fa fa-check"></i><b>10.4</b> Clustered Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="weighted-regression.html"><a href="weighted-regression.html"><i class="fa fa-check"></i><b>11</b> Weighted Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="weighted-regression.html"><a href="weighted-regression.html#weighted-least-squares-wls"><i class="fa fa-check"></i><b>11.1</b> Weighted Least Squares (WLS)</a></li>
<li class="chapter" data-level="11.2" data-path="weighted-regression.html"><a href="weighted-regression.html#when-should-you-use-wls"><i class="fa fa-check"></i><b>11.2</b> When should you use WLS?</a></li>
<li class="chapter" data-level="11.3" data-path="weighted-regression.html"><a href="weighted-regression.html#correcting-for-known-heteroskedasticity"><i class="fa fa-check"></i><b>11.3</b> Correcting for Known Heteroskedasticity</a></li>
<li class="chapter" data-level="11.4" data-path="weighted-regression.html"><a href="weighted-regression.html#sampling-weights"><i class="fa fa-check"></i><b>11.4</b> Sampling Weights</a></li>
<li class="chapter" data-level="11.5" data-path="weighted-regression.html"><a href="weighted-regression.html#references"><i class="fa fa-check"></i><b>11.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="discrete-outcome-variables.html"><a href="discrete-outcome-variables.html"><i class="fa fa-check"></i><b>12</b> Discrete Outcome Variables</a><ul>
<li class="chapter" data-level="12.1" data-path="discrete-outcome-variables.html"><a href="discrete-outcome-variables.html#linear-probability-model"><i class="fa fa-check"></i><b>12.1</b> Linear Probability Model</a></li>
<li class="chapter" data-level="12.2" data-path="discrete-outcome-variables.html"><a href="discrete-outcome-variables.html#logit-model"><i class="fa fa-check"></i><b>12.2</b> Logit Model</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="robust-regression.html"><a href="robust-regression.html"><i class="fa fa-check"></i><b>13</b> Robust Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="robust-regression.html"><a href="robust-regression.html#prerequites"><i class="fa fa-check"></i><b>13.1</b> Prerequites</a></li>
<li class="chapter" data-level="13.2" data-path="robust-regression.html"><a href="robust-regression.html#examples"><i class="fa fa-check"></i><b>13.2</b> Examples</a></li>
<li class="chapter" data-level="13.3" data-path="robust-regression.html"><a href="robust-regression.html#notes-1"><i class="fa fa-check"></i><b>13.3</b> Notes</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="bootstrapping.html"><a href="bootstrapping.html"><i class="fa fa-check"></i><b>14</b> Bootstrapping</a></li>
<li class="chapter" data-level="15" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html"><i class="fa fa-check"></i><b>15</b> Prediction and Model Comparison</a><ul>
<li class="chapter" data-level="15.1" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#prerequisites-3"><i class="fa fa-check"></i><b>15.1</b> Prerequisites</a></li>
<li class="chapter" data-level="15.2" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#measures-of-prediction"><i class="fa fa-check"></i><b>15.2</b> Measures of Prediction</a></li>
<li class="chapter" data-level="15.3" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#model-comparison"><i class="fa fa-check"></i><b>15.3</b> Model Comparison</a></li>
<li class="chapter" data-level="15.4" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#example-predicting-the-price-of-wine"><i class="fa fa-check"></i><b>15.4</b> Example: Predicting the Price of Wine</a></li>
<li class="chapter" data-level="15.5" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#cross-validation"><i class="fa fa-check"></i><b>15.5</b> Cross-Validation</a></li>
<li class="chapter" data-level="15.6" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#out-of-sample-error"><i class="fa fa-check"></i><b>15.6</b> Out of Sample Error</a><ul>
<li class="chapter" data-level="15.6.1" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#held-out-data"><i class="fa fa-check"></i><b>15.6.1</b> Held-out data</a></li>
<li class="chapter" data-level="15.6.2" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>15.6.2</b> Leave-One-Out Cross-Validation</a></li>
<li class="chapter" data-level="15.6.3" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>15.6.3</b> k-fold Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#analytic-covariance-methods"><i class="fa fa-check"></i><b>15.7</b> Analytic Covariance Methods</a></li>
<li class="chapter" data-level="15.8" data-path="prediction-and-model-comparison.html"><a href="prediction-and-model-comparison.html#further-resources"><i class="fa fa-check"></i><b>15.8</b> Further Resources</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="miscellaneous-regression-stuff.html"><a href="miscellaneous-regression-stuff.html"><i class="fa fa-check"></i><b>16</b> Miscellaneous Regression Stuff</a><ul>
<li class="chapter" data-level="16.1" data-path="miscellaneous-regression-stuff.html"><a href="miscellaneous-regression-stuff.html#anscombe-quartet"><i class="fa fa-check"></i><b>16.1</b> Anscombe quartet</a></li>
<li class="chapter" data-level="16.2" data-path="miscellaneous-regression-stuff.html"><a href="miscellaneous-regression-stuff.html#correlation-plots"><i class="fa fa-check"></i><b>16.2</b> Correlation Plots</a></li>
</ul></li>
<li class="part"><span><b>V Programming</b></span></li>
<li class="chapter" data-level="17" data-path="rs-forumula-syntax.html"><a href="rs-forumula-syntax.html"><i class="fa fa-check"></i><b>17</b> R’s Forumula Syntax</a><ul>
<li class="chapter" data-level="17.1" data-path="rs-forumula-syntax.html"><a href="rs-forumula-syntax.html#setup"><i class="fa fa-check"></i><b>17.1</b> Setup</a></li>
<li class="chapter" data-level="17.2" data-path="rs-forumula-syntax.html"><a href="rs-forumula-syntax.html#introduction-to-formula-objects"><i class="fa fa-check"></i><b>17.2</b> Introduction to Formula Objects</a></li>
<li class="chapter" data-level="17.3" data-path="rs-forumula-syntax.html"><a href="rs-forumula-syntax.html#programming-with-formulas"><i class="fa fa-check"></i><b>17.3</b> Programming with Formulas</a></li>
</ul></li>
<li class="part"><span><b>VI Examples</b></span></li>
<li class="chapter" data-level="18" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html"><i class="fa fa-check"></i><b>18</b> Duncan Occupational Prestige</a><ul>
<li class="chapter" data-level="18.1" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#setup-1"><i class="fa fa-check"></i><b>18.1</b> Setup</a></li>
<li class="chapter" data-level="18.2" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#coefficients-standard-errors"><i class="fa fa-check"></i><b>18.2</b> Coefficients, Standard errors</a></li>
<li class="chapter" data-level="18.3" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#residuals-fitted-values"><i class="fa fa-check"></i><b>18.3</b> Residuals, Fitted Values,</a></li>
<li class="chapter" data-level="18.4" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#broom"><i class="fa fa-check"></i><b>18.4</b> Broom</a></li>
<li class="chapter" data-level="18.5" data-path="duncan-occupational-prestige.html"><a href="duncan-occupational-prestige.html#plotting-fitted-regression-results"><i class="fa fa-check"></i><b>18.5</b> Plotting Fitted Regression Results</a></li>
</ul></li>
<li class="part"><span><b>VII Presentation</b></span></li>
<li class="chapter" data-level="19" data-path="formatting-tables.html"><a href="formatting-tables.html"><i class="fa fa-check"></i><b>19</b> Formatting Tables</a><ul>
<li class="chapter" data-level="19.1" data-path="formatting-tables.html"><a href="formatting-tables.html#overview-of-packages"><i class="fa fa-check"></i><b>19.1</b> Overview of Packages</a></li>
<li class="chapter" data-level="19.2" data-path="formatting-tables.html"><a href="formatting-tables.html#summary-statistic-table-example"><i class="fa fa-check"></i><b>19.2</b> Summary Statistic Table Example</a></li>
<li class="chapter" data-level="19.3" data-path="formatting-tables.html"><a href="formatting-tables.html#regression-table-example"><i class="fa fa-check"></i><b>19.3</b> Regression Table Example</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="reproducible-research.html"><a href="reproducible-research.html"><i class="fa fa-check"></i><b>20</b> Reproducible Research</a></li>
<li class="chapter" data-level="21" data-path="writing-resources.html"><a href="writing-resources.html"><i class="fa fa-check"></i><b>21</b> Writing Resources</a><ul>
<li class="chapter" data-level="21.1" data-path="writing-resources.html"><a href="writing-resources.html#writing-and-organizing-papers"><i class="fa fa-check"></i><b>21.1</b> Writing and Organizing Papers</a></li>
<li class="chapter" data-level="21.2" data-path="writing-resources.html"><a href="writing-resources.html#finding-research-ideas"><i class="fa fa-check"></i><b>21.2</b> Finding Research Ideas</a></li>
<li class="chapter" data-level="21.3" data-path="writing-resources.html"><a href="writing-resources.html#replications"><i class="fa fa-check"></i><b>21.3</b> Replications</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="multivariate-normal-distribution.html"><a href="multivariate-normal-distribution.html"><i class="fa fa-check"></i><b>A</b> Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="outliers" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Outliers</h1>
<p>This has absolutely nothing to do with the Malcolm Gladwell book.</p>
<ul>
<li>An <em>outlier</em> is an observation which has large regression errors <span class="math inline">\(\hat{\epsilon}^2\)</span>.</li>
<li>It is distant from the other observations on the <span class="math inline">\(y\)</span> dimension.</li>
<li>It increases standard errors by increasing <span class="math inline">\(\hat{\sigma}^2\)</span>, but does not bias <span class="math inline">\(\beta\)</span> if it is has typical values of <span class="math inline">\(x\)</span></li>
</ul>
<p>There are two types of extreme values in a regression.</p>
<ul>
<li>Leverage point: extreme in the <span class="math inline">\(x\)</span> direction</li>
<li>Outlier : extreme in the <span class="math inline">\(y\)</span> direction. The point has a large error (the regression line does not fit the point well)</li>
</ul>
<p>For a point to affect the results of a regression (<strong>influential</strong>) it must be both a high <em>levarage point</em> and an <em>outlier</em>.</p>
<p>The points that are <strong>influential</strong> follows from the same calculations that were in the discussion of how the linear regression is a weighted averge of points.</p>
<p>What does this mean?</p>
<ul>
<li>Are the outliers bad data?</li>
<li>Are the data truly contaminated, meaning that they come from a different distribution. This means that you are fitting the wrong model to the DGP causing inefficiency and maybe bias.</li>
</ul>
<p>Hat matrix</p>
<p>The hat matrix is named as such because it puts the “hat” on <span class="math inline">\(Y\)</span>,</p>
<p>The hat matrix <span class="math display">\[
\mat{H} = \mat{X} (\mat{X}\T \mat{X})^{-1} \mat{X}\T
\]</span></p>
<p><span class="math display">\[
\begin{aligned}[t]
\hat{\vec{\epsilon}} &amp;= \vec{y} - \mat{X} \hat{\vec{\beta}} \\
&amp;= \vec{y} - \mat{X} (\mat{X}\T \mat{X})^{-1} \mat{X} \vec{y} \\ 
&amp;= \vec{y} - \mat{H} \vec{y} \\
&amp;= (\mat{I} - \mat{H}) \vec{y}
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\hat{\vec{y}} = \mat{H} \vec{y}
\]</span></p>
<p>Some notes:</p>
<ul>
<li><span class="math inline">\(\mat{H}\)</span> is a <span class="math inline">\(n \times n\)</span> symmetric matrix</li>
<li><span class="math inline">\(\mat{H}\)</span> is idempotent: <span class="math inline">\(\mat{H} \mat{H} = \mat{H}\)</span></li>
</ul>
<p>Since <span class="math display">\[
\hat{\vec{y}} = \mat{X} \widehat{\vec{\beta}} = \mat{X} (\mat{X}\T \mat{X})^{-1} \mat{X}\T \vec{y} = \mat{H} \vec{y},
\]</span> for a particular observation <span class="math inline">\(i\)</span>, <span class="math display">\[
\hat{y}_i = \sum_{j = 1}^n h_{ij} y_j.
\]</span> The equation above means that predicted value of every observation is a weighted value of the outcomes of other observations.</p>
<p>The hat values <span class="math inline">\(h_i = h_ij\)</span> are diagonal entries in the hat matrix.</p>
<p>For a bivariate linear regresion, <span class="math display">\[
h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{j = 1}^n (x_j - \bar{x})^2},
\]</span> meaning</p>
<ul>
<li>hat values are always <em>at least</em> <span class="math inline">\(1 / n\)</span></li>
<li>hat values are a function of how far <span class="math inline">\(i\)</span> is from the center of <span class="math inline">\(\mat{X}\)</span> distribution</li>
</ul>
<p>Rule of thumb: examine hat values greater than <span class="math inline">\(2 (k + 1) / n\)</span>.</p>
<p>This example will use the following</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;MASS&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;dplyr&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;tidyr&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;broom&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;boot&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;ggplot2&quot;</span>)</code></pre></div>
<p>This ensures that we always use the <code>select</code> function from <strong>dplyr</strong> rather than the one from <strong>MASS</strong>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">select &lt;-<span class="st"> </span>dplyr<span class="op">::</span>select</code></pre></div>
<p>For the <strong>ggplot2</strong> plots, we will the default theme settings here, so that we can reuse them for all plots, and also, if we feel like changing them, we only need to change them in one location.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">theme_local &lt;-<span class="st"> </span>theme_minimal</code></pre></div>
<div id="iver-and-soskice-data" class="section level2">
<h2><span class="header-section-number">9.1</span> Iver and Soskice Data</h2>
<p>This is an example of from Iversen and Soskice (2003). That paper is interested in the relationship between party systems and redistributive efforts of the government.</p>
<p>The party system is measured using the effective number of parties; the redistributive efforts of the government is measured as the percent people lifted from poverty by taxes and transfers</p>
<p>First, let’s load the data</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iver &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;http://uw-pols503.github.io/2015/data/iver.csv&quot;</span>)
<span class="kw">glimpse</span>(iver)</code></pre></div>
<pre><code>## Observations: 14
## Variables: 8
## $ cty      &lt;fctr&gt; Australia, Belgium, Canada, Denmark, Finland, France...
## $ elec_sys &lt;fctr&gt; maj, pr, maj, pr, pr, maj, maj, pr, pr, pr, pr, unam...
## $ povred   &lt;dbl&gt; 42.16, 78.79, 29.90, 71.54, 69.08, 57.91, 46.90, 42.8...
## $ enp      &lt;dbl&gt; 2.38, 7.01, 1.69, 5.04, 5.14, 2.68, 3.16, 4.11, 3.49,...
## $ lnenp    &lt;dbl&gt; 0.867100, 1.947340, 0.524729, 1.617410, 1.637050, 0.9...
## $ maj      &lt;int&gt; 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1
## $ pr       &lt;int&gt; 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0
## $ unam     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0</code></pre>
<p>The variables of interest are <code>lnemp</code> (log effective number of parties), and <code>povred</code> (poverty reduction). Let’s plot the relationship between them</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(iver, <span class="kw">aes</span>(<span class="dt">x =</span> lnenp, <span class="dt">y =</span> povred)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;log(Number of Effective parties)&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Poverty Reduction&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_local</span>()</code></pre></div>
<p><img src="outliers_files/figure-html/unnamed-chunk-6-1.svg" width="672" /></p>
</div>
<div id="influential-observations" class="section level2">
<h2><span class="header-section-number">9.2</span> Influential Observations</h2>
<p>What are influential points in a regression? They are points that How much would the regression line change if we deleted a the point and reran the regression?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iver_mod1 &lt;-<span class="st"> </span><span class="kw">lm</span>(povred <span class="op">~</span><span class="st"> </span>lnenp, <span class="dt">data =</span> iver)

iver_loo_regs &lt;-
<span class="st">  </span><span class="co"># Start with the iver data</span>
<span class="st">  </span>iver <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># Group by country</span>
<span class="st">  </span><span class="kw">group_by</span>(cty) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># For each country</span>
<span class="st">  </span><span class="co"># Run the regression without that country and store the coefficient values</span>
<span class="st">  </span><span class="kw">do</span>({
    <span class="kw">tidy</span>(<span class="kw">lm</span>(povred <span class="op">~</span><span class="st"> </span>lnenp, <span class="dt">data =</span> <span class="kw">filter</span>(iver, cty <span class="op">!=</span><span class="st"> </span>.<span class="op">$</span>cty))) <span class="op">%&gt;%</span>
<span class="st">      </span><span class="kw">select</span>(term, estimate)
  }) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># Reshape the dataset so that each coefficient is in a column</span>
<span class="st">  </span><span class="kw">spread</span>(term, estimate) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># Calculate how much these slopes differ from the one with all the data</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">diff_slope =</span> lnenp <span class="op">-</span><span class="st"> </span><span class="kw">coef</span>(iver_mod1)[<span class="st">&quot;lnenp&quot;</span>],
         <span class="dt">abs_diff_slope =</span> <span class="kw">abs</span>(diff_slope)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># Sort by the difference in slopes</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="op">-</span><span class="st"> </span>abs_diff_slope)
    
iver_loo_regs</code></pre></div>
<pre><code>## Source: local data frame [14 x 5]
## Groups: cty [14]
## 
##               cty `(Intercept)`    lnenp  diff_slope abs_diff_slope
##            &lt;fctr&gt;         &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;
## 1     Switzerland      11.96341 35.84168 11.67038124    11.67038124
## 2   United States      33.00804 16.74422 -7.42708138     7.42708138
## 3         Belgium      26.39803 19.48071 -4.69058913     4.69058913
## 4         Denmark      23.62155 21.91104 -2.26025292     2.26025292
## 5  United Kingdom      18.40976 26.35067  2.17937476     2.17937476
## 6          Canada      24.46021 22.32840 -1.84289747     1.84289747
## 7         Finland      23.22400 22.44224 -1.72905264     1.72905264
## 8           Italy      21.22614 25.50988  1.33858580     1.33858580
## 9          France      19.31982 25.43240  1.26110272     1.26110272
## 10         Norway      19.66602 24.78567  0.61437307     0.61437307
## 11    Netherlands      21.06469 23.82630 -0.34499536     0.34499536
## 12         Sweden      20.93623 24.04618 -0.12511217     0.12511217
## 13      Australia      21.96619 24.07283 -0.09847140     0.09847140
## 14        Germany      22.08429 24.10787 -0.06343038     0.06343038</code></pre>
<p>Switzerland looks particularly problematic. The effect of <code>lnenp</code> on <code>povred</code> is 7.</p>
<p>We could also plot these lines against the original data, to get a more intuitive sense of how much dropping one observation affects the regression slopes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">data =</span> iver_loo_regs, <span class="kw">aes</span>(<span class="dt">intercept =</span> <span class="st">`</span><span class="dt">(Intercept)</span><span class="st">`</span>,
                                        <span class="dt">slope =</span> lnenp)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> iver, <span class="kw">aes</span>(<span class="dt">x =</span> lnenp, <span class="dt">y =</span> povred)) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;log(Number of Effective parties)&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Poverty Reduction&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_local</span>()</code></pre></div>
<p><img src="outliers_files/figure-html/unnamed-chunk-8-1.svg" width="672" /></p>
<p>Conveniently, in linear regression we can find which observations will have the largest influence on regression lines without rerunning the regression. Three statistics are of interest:</p>
<ul>
<li>Cook’s distance: a single number that summarizes how much dropping an observation changes <strong>all</strong> the regression coefficients.</li>
<li>Studentized residual: The scaled residual of the observation.</li>
<li>Hat score: How far the observation is from the center of the data.</li>
</ul>
<p>Use the <strong>broom</strong> function augment to add residuals and other diagnostic data to the original regression data. See <code>help(influence)</code> for functions to get these diagnostics using base R.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iver_mod1_aug &lt;-<span class="st"> </span><span class="kw">augment</span>(iver_mod1) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">cty =</span> iver<span class="op">$</span>cty)
<span class="kw">glimpse</span>(iver_mod1_aug)</code></pre></div>
<pre><code>## Observations: 14
## Variables: 10
## $ povred     &lt;dbl&gt; 42.16, 78.79, 29.90, 71.54, 69.08, 57.91, 46.90, 42...
## $ lnenp      &lt;dbl&gt; 0.867100, 1.947340, 0.524729, 1.617410, 1.637050, 0...
## $ .fitted    &lt;dbl&gt; 42.75835, 68.86916, 34.48280, 60.89432, 61.36904, 4...
## $ .se.fit    &lt;dbl&gt; 6.692466, 10.833460, 10.047244, 7.413818, 7.595310,...
## $ .resid     &lt;dbl&gt; -0.5983544, 9.9208441, -4.5828034, 10.6456800, 7.71...
## $ .hat       &lt;dbl&gt; 0.11973167, 0.31374085, 0.26985506, 0.14693340, 0.1...
## $ .sigma     &lt;dbl&gt; 20.20023, 19.87581, 20.13632, 19.89997, 20.04234, 1...
## $ .cooksd    &lt;dbl&gt; 7.394393e-05, 8.763926e-02, 1.420959e-02, 3.058498e...
## $ .std.resid &lt;dbl&gt; -0.03297382, 0.61918852, -0.27729691, 0.59593691, 0...
## $ cty        &lt;fctr&gt; Australia, Belgium, Canada, Denmark, Finland, Fran...</code></pre>
<p>Oddly, <code>augment</code> calculates the <em>standardized residual</em>, <span class="math display">\[
\mathtt{.std.resid} = E&#39;_i = \frac{E_i}{S_E \sqrt{1 - h_i}}
\]</span> which divides by the regression residual standard error, which is itself a function of the residual of <span class="math inline">\(i\)</span>, <span class="math inline">\(S_E = \sqrt{\frac{\sum_j E_j}{n - k - 1}}\)</span>. What we want is the <em>studentized residual</em> which divides by the standard error of the regression calculated omitting observation <span class="math inline">\(i\)</span>: <span class="math display">\[
\mathtt{.resid / .sigma * sqrt(1 - .hat)} = E^*_i = \frac{E_i}{S_{E_{(i)}} \sqrt{1 - h_i}}
\]</span> where <span class="math inline">\(S_{E_(i)}\)</span> is the standard error of the regression run without observation <span class="math inline">\(i\)</span>. It is called the Studentized residual, because it is distributed Student’s <span class="math inline">\(t\)</span>; the standardized residual is not. Add a new variable called <code>.student.resid</code>, which we can calculate using the residual (<code>.resid</code>), standard error of the regression that omits that observation (<code>.sigma</code>), and the hat value (<code>.hat</code>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iver_mod1_aug &lt;-
<span class="st">  </span>iver_mod1_aug <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">.student.resid =</span> .resid <span class="op">/</span><span class="st"> </span>.sigma <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>.hat))</code></pre></div>
<p>In base R, the function <code>rstudent</code> calculates the Studentized residuals, and <code>rstandard</code> calculates the standardized residuals:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">setNames</span>(<span class="kw">rstudent</span>(iver_mod1), iver<span class="op">$</span>cty)</code></pre></div>
<pre><code>##      Australia        Belgium         Canada        Denmark        Finland 
##    -0.03157146     0.60253131    -0.26634629     0.57920127     0.41834057 
##         France        Germany          Italy    Netherlands         Norway 
##     0.64999275    -0.13942917    -0.69795162     0.78818851     0.97001936 
##         Sweden    Switzerland United Kingdom  United States 
##     0.69123825    -4.39123120     0.49519482    -1.57878326</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">setNames</span>(<span class="kw">rstandard</span>(iver_mod1), iver<span class="op">$</span>cty)</code></pre></div>
<pre><code>##      Australia        Belgium         Canada        Denmark        Finland 
##    -0.03297382     0.61918852    -0.27729691     0.59593691     0.43350755 
##         France        Germany          Italy    Netherlands         Norway 
##     0.66622163    -0.14550050    -0.71336214     0.80092982     0.97241536 
##         Sweden    Switzerland United Kingdom  United States 
##     0.70678751    -2.76425506     0.51154375    -1.48890165</code></pre>
<p>This scatterplot weights observations by their hat score. Points further from the mean of <code>lnenp</code> have higher hat scores.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="dt">data =</span> iver_mod1_aug, <span class="kw">aes</span>(<span class="dt">x =</span> lnenp, <span class="dt">y =</span> povred)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">size =</span> .hat)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_local</span>()</code></pre></div>
<p><img src="outliers_files/figure-html/unnamed-chunk-12-1.svg" width="672" /></p>
<p>This scatterplot weights observations by their absolute Studentized residuals. Those observations furthest from the regression line <em>and</em> high hat values, have the highest residuals.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="dt">data =</span> iver_mod1_aug, <span class="kw">aes</span>(<span class="dt">x =</span> lnenp, <span class="dt">y =</span> povred)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">size =</span> <span class="kw">abs</span>(.student.resid))) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_local</span>()</code></pre></div>
<p><img src="outliers_files/figure-html/unnamed-chunk-13-1.svg" width="672" /> Cook’s distance is a measure of the overall influence of points on the regression; the point’s effect on <em>all</em> the parameters. This plot weights points by their Cook’s distance. We can see that the two points on the bottom (Switzerland and the US) have the highest Cook’s distance.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="dt">data =</span> iver_mod1_aug, <span class="kw">aes</span>(<span class="dt">x =</span> lnenp, <span class="dt">y =</span> povred)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">size =</span> .cooksd)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_local</span>()</code></pre></div>
<p><img src="outliers_files/figure-html/unnamed-chunk-14-1.svg" width="672" /></p>
<p>A standard plot to assess outliers is the Influence Plot. The x-axis is hat scores, the y-axis is Studentized residuals. The points are sized by Cook’s Distance. Rules of thumb lines are drawn at -2 and 2 for Studentized residuals, and <span class="math inline">\(\bar{h} + 2 sd(h)\)</span> and <span class="math inline">\(\bar{h} + 3 sd(h)\)</span> for hat scores.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> iver_mod1_aug,
             <span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> .hat, <span class="dt">y =</span> .student.resid, <span class="dt">size =</span> .cooksd)) <span class="op">+</span>
<span class="st">  </span><span class="co"># add labels to points, but only those points that are flagged as outliers</span>
<span class="st">  </span><span class="co"># for at least one of the diagnostics considered here</span>
<span class="st">  </span><span class="kw">geom_text</span>(<span class="dt">data =</span>
              <span class="kw">filter</span>(iver_mod1_aug,
                     .cooksd <span class="op">&gt;</span><span class="st"> </span><span class="dv">4</span> <span class="op">/</span><span class="st"> </span>iver_mod1<span class="op">$</span>df.residual
                     <span class="op">|</span><span class="st"> </span><span class="kw">abs</span>(.student.resid) <span class="op">&gt;</span><span class="st"> </span><span class="dv">2</span>
                     <span class="op">|</span><span class="st"> </span>.hat <span class="op">&gt;</span><span class="st"> </span><span class="kw">mean</span>(.hat) <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(.hat)),
            <span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> .hat, <span class="dt">y =</span> .student.resid, <span class="dt">label =</span> cty),
            <span class="dt">hjust =</span> <span class="dv">0</span>, <span class="dt">size =</span> <span class="dv">4</span>, <span class="dt">colour =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">yintercept =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>)),
             <span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">yintercept =</span> yintercept),
             <span class="dt">colour =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.4</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(iver_mod1_aug<span class="op">$</span>.hat) <span class="op">+</span>
<span class="st">                                 </span><span class="kw">sd</span>(iver_mod1_aug<span class="op">$</span>.hat) <span class="op">*</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>)),
             <span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">xintercept =</span> xintercept),
             <span class="dt">colour =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.4</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;hat&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Studentized residuals&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_size_continuous</span>(<span class="st">&quot;Cook&#39;s Distance&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme_local</span>()</code></pre></div>
<p><img src="outliers_files/figure-html/unnamed-chunk-15-1.svg" width="576" /></p>
<p>Instead of a plot, we could find the id Observations with high Cook’s distance (greater than <span class="math inline">\(4 / (n - k - 1)\)</span>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">filter</span>(iver_mod1_aug, .cooksd <span class="op">&gt;</span><span class="st"> </span>(<span class="dv">4</span> <span class="op">/</span><span class="st"> </span>iver_mod1<span class="op">$</span>df.residual)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(cty, .cooksd, lnenp)</code></pre></div>
<pre><code>##           cty  .cooksd   lnenp
## 1 Switzerland 0.745124 1.66013</code></pre>
<p>Observations with high hat scores (greater than 2 standard deviations than the mean hat score):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">filter</span>(iver_mod1_aug, .hat <span class="op">&gt;</span><span class="st"> </span><span class="kw">mean</span>(.hat) <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(.hat)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(cty, .hat, lnenp)</code></pre></div>
<pre><code>##       cty      .hat   lnenp
## 1 Belgium 0.3137408 1.94734</code></pre>
<p>Observations with high Studentized residuals (+/- 2):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">filter</span>(iver_mod1_aug, <span class="kw">abs</span>(.student.resid) <span class="op">&gt;</span><span class="st"> </span><span class="dv">2</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(cty, .student.resid, lnenp)</code></pre></div>
<pre><code>##           cty .student.resid   lnenp
## 1 Switzerland      -3.674577 1.66013</code></pre>
<p>Or combine these,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">filter</span>(iver_mod1_aug,
       <span class="kw">abs</span>(.student.resid) <span class="op">&gt;</span><span class="st"> </span><span class="dv">2</span> <span class="op">|</span>
<span class="st">         </span>.hat <span class="op">&gt;</span><span class="st"> </span><span class="kw">mean</span>(.hat) <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(.hat) <span class="op">|</span>
<span class="st">         </span>.cooksd <span class="op">&gt;</span><span class="st"> </span><span class="dv">4</span> <span class="op">/</span><span class="st"> </span>iver_mod1<span class="op">$</span>df.residual) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(cty, .cooksd, .hat, .student.resid, lnenp)</code></pre></div>
<pre><code>##           cty    .cooksd      .hat .student.resid   lnenp
## 1     Belgium 0.08763926 0.3137408      0.4134926 1.94734
## 2 Switzerland 0.74512398 0.1632012     -3.6745770 1.66013</code></pre>
<p>Also see <code>influencePlot</code> in <strong>car</strong>, and <code>influencePlot</code> in <strong>simcf</strong> for other implementations of this plot type. One feature of those implementations is that they allow for the ability to identify the points on the plot.</p>
<p>Now that we’ve identified Switzerland as a problematic point, the question is what to do about it. Checking the Switzerland data, it appears that it is correct and is not the result of data entry issues. In general, we should avoid dropping points. Perhaps the issue is that we have not accounted for different electoral systems. Let’s try including</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iver_mod2 &lt;-<span class="st"> </span><span class="kw">lm</span>(povred <span class="op">~</span><span class="st"> </span>lnenp <span class="op">+</span><span class="st"> </span>elec_sys, <span class="dt">data =</span> iver)
iver_mod2</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = povred ~ lnenp + elec_sys, data = iver)
## 
## Coefficients:
##  (Intercept)         lnenp    elec_syspr  elec_sysunam  
##       17.658        26.693         9.221       -48.952</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iver_mod2_aug &lt;-<span class="st"> </span><span class="kw">augment</span>(iver_mod2) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">.student.resid =</span> .resid <span class="op">/</span><span class="st"> </span>(.sigma <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>.hat)),
         <span class="dt">cty =</span> iver<span class="op">$</span>cty)</code></pre></div>
<p>However, by including a categorical variable for electoral system in which Switzerland is the only country with a unanamity government, we are effectively dropping Switzerland from the regression. This means that we cannot calculate Cook’s distance or studentized residuals, or hat scores for Switzerland since a regression estimated <em>without</em> switzerland cannot estimate a coefficient for the <code>unam</code> category, since Switzerland is the only member of that category.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">filter</span>(iver_mod2_aug,
       <span class="kw">abs</span>(.student.resid) <span class="op">&gt;</span><span class="st"> </span><span class="dv">2</span> <span class="op">|</span>
<span class="st">         </span>.hat <span class="op">&gt;</span><span class="st"> </span><span class="kw">mean</span>(.hat) <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(.hat) <span class="op">|</span>
<span class="st">         </span>.cooksd <span class="op">&gt;</span><span class="st"> </span><span class="dv">4</span> <span class="op">/</span><span class="st"> </span>iver_mod1<span class="op">$</span>df.residual) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(cty, .cooksd, .hat, .student.resid, lnenp)</code></pre></div>
<pre><code>##             cty   .cooksd      .hat .student.resid    lnenp
## 1         Italy 0.1548996 0.1455977      -2.267585 1.413420
## 2   Switzerland       NaN 1.0000000           -Inf 1.660130
## 3 United States 0.2749067 0.1978831      -2.690288 0.667829</code></pre>
<p>But now that we’ve ignored Switzerland, both Italy and the United States seem to be influential. This is because now that there are fewer observations per group, in some sense it is easier for observations to be influentia. But, although the US and Italy have high studentized residuals, neither of them exceed the rule of thumb for Cooks distance.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">filter</span>(iver_mod2_aug,
         .cooksd <span class="op">&gt;</span><span class="st"> </span><span class="dv">4</span> <span class="op">/</span><span class="st"> </span>iver_mod1<span class="op">$</span>df.residual) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(cty, .cooksd, .hat, .student.resid, lnenp)</code></pre></div>
<pre><code>## [1] cty            .cooksd        .hat           .student.resid
## [5] lnenp         
## &lt;0 rows&gt; (or 0-length row.names)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> <span class="kw">filter</span>(iver_mod2_aug, .cooksd <span class="op">&lt;</span><span class="st"> </span><span class="ot">Inf</span>),
             <span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> .hat, <span class="dt">y =</span> .student.resid, <span class="dt">size =</span> .cooksd)) <span class="op">+</span>
<span class="st">  </span><span class="co"># add labels to points, but only those points that are flagged as outliers</span>
<span class="st">  </span><span class="co"># for at least one of the diagnostics considered here</span>
<span class="st">  </span><span class="kw">geom_text</span>(<span class="dt">data =</span>
              <span class="kw">filter</span>(iver_mod2_aug,
                     .cooksd <span class="op">&gt;</span><span class="st"> </span><span class="dv">4</span> <span class="op">/</span><span class="st"> </span>iver_mod2<span class="op">$</span>df.residual
                     <span class="op">|</span><span class="st"> </span><span class="kw">abs</span>(.student.resid) <span class="op">&gt;</span><span class="st"> </span><span class="dv">2</span>
                     <span class="op">|</span><span class="st"> </span>.hat <span class="op">&gt;</span><span class="st"> </span><span class="kw">mean</span>(.hat) <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(.hat),
                     .cooksd <span class="op">&lt;</span><span class="st"> </span><span class="ot">Inf</span>),
            <span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> .hat, <span class="dt">y =</span> .student.resid, <span class="dt">label =</span> cty),
            <span class="dt">hjust =</span> <span class="dv">0</span>, <span class="dt">size =</span> <span class="dv">4</span>, <span class="dt">colour =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">yintercept =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>)),
             <span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">yintercept =</span> yintercept),
             <span class="dt">colour =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.4</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(iver_mod2_aug<span class="op">$</span>.hat) <span class="op">+</span>
<span class="st">                                 </span><span class="kw">sd</span>(iver_mod2_aug<span class="op">$</span>.hat) <span class="op">*</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>)),
             <span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">xintercept =</span> xintercept),
             <span class="dt">colour =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.4</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;hat&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Studentized residuals&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_size_continuous</span>(<span class="st">&quot;Cook&#39;s Distance&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme_local</span>()</code></pre></div>
<p><img src="outliers_files/figure-html/unnamed-chunk-23-1.svg" width="576" /></p>
<p>Although there are still a few observations with large residuals, and with a small dataset, it is almost inevitable that some observations will have outsized influence on the results, from an outlier perspective the new model seems less problematic. However, we accomplished this at the cost of effectively ignoring Switzerland. The model is able to estimate how different Switzerland is from what would be predicted, but by including a dummy variable that is only 1 for Switzerland, we are treating Switzerland as <em>sui generis</em>. Also note, that although the category is called <code>unam</code>, it would be inappropriate to interpret it as the effect of that type of government since Switzerland is the only country in that category. We cannot separate the effect of the government type from all the other things that make Switzerland unique. It would be more appropriate to call it the “Switzerland” category in this instance.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="omitted-variable-bias.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="problems-with-errors.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jrnold/intro-method-notes/edit/master/outliers.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
